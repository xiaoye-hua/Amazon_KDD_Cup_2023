{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfb20369-bc15-496e-9ca7-96ea3337a84e",
   "metadata": {},
   "source": [
    "# Packages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aabe0c4e-8729-4f63-9722-c12d233ca6b5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/.cache/pypoetry/virtualenvs/kdd-2023-KklMGVX0-py3.8/lib/python3.8/site-packages/implicit/gpu/__init__.py:13: UserWarning: CUDA extension is built, but disabling GPU support because of 'Cuda Error: no CUDA-capable device is detected (/project/./implicit/gpu/utils.h:71)'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "import logging\n",
    "base_dir = '../'\n",
    "sys.path.append(base_dir)\n",
    "import os\n",
    "from utils import *\n",
    "\n",
    "import joblib\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.similarities.annoy import AnnoyIndexer\n",
    "\n",
    "\n",
    "from annoy import AnnoyIndex\n",
    "import polars as pl\n",
    "import implicit\n",
    "from src.eval import model_eval\n",
    "\n",
    "import scipy.sparse as sps\n",
    "from utils import str2list\n",
    "from src.config import raw_data_session_id_dir, candidate_file_name\n",
    "from lightgbm import LGBMRanker\n",
    "from lightgbm import early_stopping\n",
    "from utils import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1355ce62-0ebd-4629-99e5-96bf2946bed5",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Config "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "581b9e23-e0c5-4956-9aee-45b64c96f201",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{task}_{data_type}_{model_version}_{model_for_eval}_top{topn}.parquet'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candidate_file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0cac5f9f-76a0-4069-a57a-17b45bba83fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "debug = False\n",
    "\n",
    "eval_cg = False\n",
    "\n",
    "load_train_eval_data = False\n",
    "if debug:\n",
    "    SAMPLE_NUM = 1000\n",
    "else:\n",
    "    SAMPLE_NUM = None\n",
    "\n",
    "candidate_path = '../data/candidates/'\n",
    "model_dir = '../model_training'\n",
    "ranker_train_data_dir = '../data/rank_train_data'\n",
    "# train_data_dir = '.'\n",
    "# test_data_dir = '.'\n",
    "task = 'task1'\n",
    "w2v_model_version = 'w2v_v3'\n",
    "nic_model_version = 'nic'\n",
    "# rank_model_version = 'rank_lgbm_v2'\n",
    "\n",
    "# rank_model_dir = os.path.join(model_dir, rank_model_version)\n",
    "model_for_eval = True\n",
    "w2v_topn=100\n",
    "nic_topn=100\n",
    "# PREDS_PER_SESSION = 100\n",
    "\n",
    "# num_tree = 100\n",
    "# # target locales: locales needed for task1\n",
    "target_locals = [\"DE\", 'JP', 'UK']\n",
    "\n",
    "# submit_file = f'submission_{task}_ALS.parquet'\n",
    "num_tree = 100\n",
    "w2v_model_dir = os.path.join(model_dir, w2v_model_version)\n",
    "w2v_model_file = os.path.join(w2v_model_dir, f\"{model_for_eval}.model\")\n",
    "annoy_index_file = os.path.join(w2v_model_dir, f\"{str(num_tree)}_{model_for_eval}.index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902bc312-8c20-42f7-ad41-b5d77d89fcce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "210036e4-72ad-4a2d-b704-5df89adede53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘../data/rank_train_data’: File exists\n"
     ]
    }
   ],
   "source": [
    "! mkdir {ranker_train_data_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "701392f5-757d-4404-8c58-0cc400b5e474",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33cccd9d-18fd-4339-a739-39d7021cef73",
   "metadata": {},
   "source": [
    "# Original data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3486f554-4ca0-460c-9d31-4b4b1f2c5e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# eval_pl = pl.scan_parquet(os.path.join(base_dir, raw_data_session_id_dir, 'sessions_eval.parquet'), n_rows=SAMPLE_NUM).filter(pl.col('locale').is_in(target_locals)).with_columns(pl.col('prev_items').apply(str2list))\n",
    "\n",
    "# # df_sess.head(3).collect()\n",
    "# test_pl = pl.scan_parquet(os.path.join(base_dir, raw_data_session_id_dir, 'sessions_test_task1.parquet'), n_rows=SAMPLE_NUM).with_columns(pl.col('prev_items').apply(str2list))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e9d66b7-afd4-4207-a497-d733fe520055",
   "metadata": {},
   "source": [
    "# Get candiadtes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5242ea97-e5dd-45aa-96ef-043d295133fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_candidates(data_type, task=task, \n",
    "                       w2v_model_version=w2v_model_version, \n",
    "                       nic_model_version=nic_model_version\n",
    "                      ,model_for_eval=model_for_eval,\n",
    "                      w2v_topn=w2v_topn\n",
    "                      , nic_topn=nic_topn):\n",
    "    \n",
    "    w2v_file = os.path.join(candidate_path, \n",
    "                           candidate_file_name.format(\n",
    "                    task=task\n",
    "                , data_type=data_type\n",
    "                , model_version=w2v_model_version\n",
    "                , model_for_eval=model_for_eval\n",
    "                , topn=w2v_topn\n",
    "                           ))\n",
    "    nic_file = os.path.join(candidate_path,\n",
    "                candidate_file_name.format(\n",
    "                    task=task\n",
    "                    , data_type=data_type\n",
    "                    , model_version=nic_model_version\n",
    "                    , model_for_eval=model_for_eval\n",
    "                    , topn=nic_topn\n",
    "                           ))\n",
    "    if data_type == 'test':\n",
    "        original_file_name = f\"sessions_{data_type}_{task}.parquet\"\n",
    "    else:\n",
    "        original_file_name = f\"sessions_{data_type}.parquet\"\n",
    "    original_pl = pl.scan_parquet(os.path.join(base_dir, raw_data_session_id_dir, original_file_name), n_rows=SAMPLE_NUM).filter(pl.col('locale').is_in(target_locals)).with_columns(pl.col('prev_items').apply(str2list))\n",
    "    w2v_pl = pl.scan_parquet(w2v_file, n_rows=SAMPLE_NUM)#.with_columns(pl.col('prev_items').apply(str2list))\n",
    "    nic_pl = pl.scan_parquet(nic_file, n_rows=SAMPLE_NUM)#.with_columns(pl.col('prev_items').apply(str2list))\n",
    "    \n",
    "    # get w2v weight\n",
    "    w2v_pl = w2v_pl.with_columns(\n",
    "        pl.lit([list(range(w2v_topn, 0, -1))]).alias('w2v_weight')\n",
    "    )\n",
    "    get_w2v_weight = pl.element().rank()*0\n",
    "    nic_pl = nic_pl.with_columns(\n",
    "        pl.col('next_item_prediction').arr.eval(get_w2v_weight, \n",
    "                                                # parallel=True\n",
    "                                               ).alias('w2v_weight').cast(pl.List(pl.Int64))\n",
    "    )\n",
    "    cols = ['session_id', 'next_item_prediction', 'w2v_weight']\n",
    "    combined_pl = w2v_pl.select(cols).join(nic_pl.select(cols), how='left', on='session_id', suffix='_nic')\n",
    "\n",
    "    # combined_pl = combined_pl.with_columns(\n",
    "    #     pl.concat_list([pl.col('next_item_prediction'), pl.col('next_item_prediction_nic')]).alias('next_item_prediction'), \n",
    "    #     pl.concat_list([pl.col('w2v_weight'), pl.col('w2v_weight_nic')]).alias('w2v_weight'), \n",
    "    # ).select(\n",
    "    #     pl.all().exclude(['next_item_prediction_nic', 'w2v_weight_nic'])\n",
    "    # )\n",
    "    explode_cols = ['next_item_prediction', 'w2v_weight']\n",
    "    combined_pl = (\n",
    "            pl.concat([w2v_pl.select(cols).explode(explode_cols), nic_pl.select(cols).explode(explode_cols)], how='vertical')\n",
    "                .groupby(['session_id', 'next_item_prediction'])\n",
    "                .agg(\n",
    "                    pl.col('w2v_weight').max().alias('w2v_weight')\n",
    "                )\n",
    "                .groupby('session_id')\n",
    "                .agg(\n",
    "                    pl.all()\n",
    "                )\n",
    "    )\n",
    "    combined_pl = (\n",
    "        combined_pl.join(original_pl, how='left', on='session_id')\n",
    "            # .with_columns(\n",
    "            #     pl.col('prev_items')\n",
    "            # )\n",
    "    )\n",
    "    return combined_pl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5450fc2a-d9ee-4895-b972-0033129c3193",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cg_pl = get_all_candidates(data_type='train')\n",
    "eval_cg_pl = get_all_candidates(data_type='eval')\n",
    "test_cg_pl = get_all_candidates(data_type='test')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db6d45e-36d7-4844-a312-e419b7d9ff3e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Eval candidate generation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3bcff578-861e-46af-b1f7-fab9d7a774af",
   "metadata": {},
   "outputs": [],
   "source": [
    "if eval_cg:\n",
    "    eval_topn = 300\n",
    "\n",
    "    col = f\"recall@{eval_topn}\"\n",
    "\n",
    "    eval_final = (\n",
    "            eval_cg_pl\n",
    "            .lazy()\n",
    "            .with_columns(\n",
    "                pl.col('next_item_prediction').cast(pl.List(pl.Utf8))\n",
    "            )\n",
    "            .with_columns(\n",
    "                pl.concat_list([pl.col('next_item'), pl.col('next_item_prediction')]).alias('mrr')\n",
    "            )\n",
    "            ).with_columns(\n",
    "                pl.col('next_item_prediction').arr.head(eval_topn).arr.contains(pl.col('next_item')).mean().alias(col)\n",
    "\n",
    "            )\n",
    "    final_res = eval_final.select(\n",
    "            pl.count().alias('total_sessions')\n",
    "            , pl.col(col).mean()\n",
    "\n",
    "        ).collect()\n",
    "    print(final_res)\n",
    "    del final_res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a40c22f-010f-4bba-ae1f-aa76247c638d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Feature Process "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1552490e-efa5-49fe-810f-954bdc3f6517",
   "metadata": {},
   "source": [
    "## Load NIC, W2V Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a9c64bc6-0657-455d-bf4c-f4cfe23d6e1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'next_item_prediction': Utf8, 'next_item_weight': Float64, 'last_prev_item': Utf8}\n"
     ]
    }
   ],
   "source": [
    "nic_model = (\n",
    "    pl.scan_parquet('../model_training/next_item_counter_v2/nic_model.parquet')\n",
    "        .explode(['next_item_prediction', 'next_item_weight'])\n",
    "        .select(\n",
    "            pl.all().exclude('item')\n",
    "            , pl.col('item').alias('last_prev_item')\n",
    "        )\n",
    "            )\n",
    "print(nic_model.schema)\n",
    "\n",
    "\n",
    "\n",
    "# w2v_model_file = '../model_training/v2/w2v.model'\n",
    "w2vec = Word2Vec.load(w2v_model_file)\n",
    "annoy_index = AnnoyIndexer()\n",
    "annoy_index.load(annoy_index_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7ccd61-b973-4497-9afe-0c84c7f7af9b",
   "metadata": {},
   "source": [
    "## get features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9bd3e83e-001e-4634-a12b-fdfa3e2b8b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# target_df = train_candidates\n",
    "# data_type = 'train'\n",
    "\n",
    "def get_w2v_simi(x, col='last_prev_item'):\n",
    "    try:\n",
    "        if isinstance(x[col], str):\n",
    "            target = [x[col]]\n",
    "        else:\n",
    "            target = x[col]\n",
    "        simi = w2vec.wv.n_similarity([x['next_item_prediction']],\n",
    "                                                  target\n",
    "                                                 )\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        # print(x[col])\n",
    "        simi = 0\n",
    "    return simi\n",
    "\n",
    "def get_feature(target_df, data_type, nic_model=nic_model, negative_sampling=None):\n",
    "\n",
    "    target_df = target_df.with_columns(\n",
    "                    pl.col('prev_items').arr.lengths().alias('prev_length')\n",
    "                                      ).explode(['next_item_prediction', 'w2v_weight'])\n",
    "\n",
    "    if data_type != 'test':\n",
    "        target_df = (\n",
    "            target_df\n",
    "                .with_columns(\n",
    "                    pl.when(pl.col('next_item')==pl.col('next_item_prediction')).then(1).otherwise(0).alias('target')\n",
    "                )\n",
    "        )\n",
    "\n",
    "    if negative_sampling is not None:\n",
    "        target_df = target_df.collect()\n",
    "        pos = target_df.filter(pl.col('target')==1)\n",
    "        neg = target_df.filter(pl.col('target')==0).sample(frac=negative_sampling)\n",
    "        target_df = pl.concat([pos, neg], how='vertical')\n",
    "        print(target_df.select('target').mean())\n",
    "\n",
    "    target_df = (\n",
    "            target_df.lazy().with_columns(\n",
    "                    pl.col('prev_items').arr.get(-1).alias('last_prev_item')\n",
    "                ).join(nic_model, how='left', on=['last_prev_item', 'next_item_prediction'])\n",
    "                .with_columns(\n",
    "                    pl.when(pl.col('next_item_weight').is_null()).then(0).otherwise(pl.col('next_item_weight')).alias('next_item_weight')\n",
    "                    , pl.struct([\"next_item_prediction\", \"last_prev_item\"]).apply(\n",
    "                        lambda x: get_w2v_simi(x, col='last_prev_item')).alias('last_item_similarity').cast(pl.Float32)\n",
    "                    # , pl.struct([\"next_item_prediction\", \"prev_items\"]).apply(\n",
    "                    #     lambda x: get_w2v_simi(x, col='prev_items')).alias('prev_item_similarity').cast(pl.Float32)\n",
    "                    , pl.when(pl.col('locale')=='DE').then(1).when(pl.col('locale')=='DE')\n",
    "                        .then(2)\n",
    "                        .otherwise(3).alias('locale')\n",
    "                ).sort('session_id')\n",
    "\n",
    "    )\n",
    "    return target_df\n",
    "# target_df.head(3).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "be32562e-3c44-4aa9-a119-39d78665bf40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time \n",
    "# sample_pl = get_feature(target_df=train_cg_pl, data_type='train', #negative_sampling=0.1\n",
    "#                        ).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c8d6ac47-21f7-4f25-8fe6-d2cf3fa692ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time \n",
    "# sample_pl = get_feature(target_df=train_cg_pl, data_type='train',# negative_sampling=0.1\n",
    "#                        ).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f14fa779-df68-4f18-a42b-03f3d0d301af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dfab3a0-7179-4425-85cd-e218df0c31ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_train_candidates = get_feature(target_df=train_cg_pl, data_type='train', negative_sampling=0.1)\n",
    "eval_candidates = get_feature(target_df=eval_cg_pl, data_type='eval')\n",
    "test_candidates = get_feature(target_df=test_cg_pl, data_type='test')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4e8547-6eb6-4533-83c8-b22a17f34803",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Save Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5792a3-6aa0-43be-a5c8-dda551f4c7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_train_candidates = sampled_train_candidates.collect()\n",
    "eval_candidates = eval_candidates.collect()\n",
    "test_candidates = test_candidates.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a574fc79-c656-45c3-8e95-adaddd57b978",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Sampled train data')\n",
    "print(sampled_train_candidates.shape)\n",
    "print(sampled_train_candidates['target'].mean())\n",
    "print('Eval data')\n",
    "print(eval_candidates.shape)\n",
    "print(eval_candidates['target'].mean())\n",
    "print('Test data')\n",
    "print(test_candidates.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19af477e-0d8f-4abb-b704-30c6445e94d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ranker_train_data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33163c5-ac03-435e-9812-31f4f2b8ca35",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not debug:\n",
    "    sampled_train_candidates.write_parquet(os.path.join(ranker_train_data_dir, 'train.parquet'))\n",
    "    eval_candidates.write_parquet(os.path.join(ranker_train_data_dir, 'eval.parquet'))\n",
    "    test_candidates.write_parquet(os.path.join(ranker_train_data_dir, 'test.parquet'))"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "kdd_2023",
   "name": "common-cu110.m104",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cu110:m104"
  },
  "kernelspec": {
   "display_name": "py3.8(kdd_2023)",
   "language": "python",
   "name": "kdd_2023"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
