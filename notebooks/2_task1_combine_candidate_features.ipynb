{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfb20369-bc15-496e-9ca7-96ea3337a84e",
   "metadata": {},
   "source": [
    "# Packages "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aabe0c4e-8729-4f63-9722-c12d233ca6b5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/.cache/pypoetry/virtualenvs/kdd-2023-KklMGVX0-py3.8/lib/python3.8/site-packages/implicit/gpu/__init__.py:13: UserWarning: CUDA extension is built, but disabling GPU support because of 'Cuda Error: no CUDA-capable device is detected (/project/./implicit/gpu/utils.h:71)'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "import logging\n",
    "base_dir = '../'\n",
    "sys.path.append(base_dir)\n",
    "import os\n",
    "from utils import *\n",
    "\n",
    "import joblib\n",
    "import os\n",
    "import numpy as np\n",
    "import scipy\n",
    "import pandas as pd\n",
    "from gensim.models import Word2Vec\n",
    "from tqdm import tqdm\n",
    "from gensim.similarities.annoy import AnnoyIndexer\n",
    "\n",
    "\n",
    "from annoy import AnnoyIndex\n",
    "import polars as pl\n",
    "import implicit\n",
    "from src.eval import model_eval\n",
    "\n",
    "import scipy.sparse as sps\n",
    "from utils import str2list\n",
    "from src.config import raw_data_session_id_dir, candidate_file_name\n",
    "from lightgbm import LGBMRanker\n",
    "from lightgbm import early_stopping\n",
    "from utils import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1355ce62-0ebd-4629-99e5-96bf2946bed5",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Config "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "581b9e23-e0c5-4956-9aee-45b64c96f201",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{task}_{data_type}_{model_version}_{model_for_eval}_top{topn}.parquet'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candidate_file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0cac5f9f-76a0-4069-a57a-17b45bba83fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "debug = False\n",
    "\n",
    "eval_cg = False\n",
    "\n",
    "reprocess_train_data = False\n",
    "train_parts = 20\n",
    "\n",
    "load_train_eval_data = False\n",
    "if debug:\n",
    "    SAMPLE_NUM = 1000\n",
    "else:\n",
    "    SAMPLE_NUM = None\n",
    "\n",
    "\n",
    "candidate_path = '../data/candidates/'\n",
    "model_dir = '../model_training'\n",
    "ranker_train_data_dir = '../data/rank_train_data_v2'\n",
    "# train_data_dir = '.'\n",
    "# test_data_dir = '.'\n",
    "task = 'task1'\n",
    "w2v_model_version = 'w2v_v3'\n",
    "nic_model_version = 'nic'\n",
    "nfi_model_version = 'next_few_items_v1'\n",
    "covisit_model_version = 'co_visit'\n",
    "\n",
    "# rank_model_dir = os.path.join(model_dir, rank_model_version)\n",
    "model_for_eval = True\n",
    "w2v_topn=200\n",
    "nic_topn=100\n",
    "nfi_topn=100\n",
    "covisit_topn = 300\n",
    "# PREDS_PER_SESSION = 100\n",
    "\n",
    "# num_tree = 100\n",
    "# # target locales: locales needed for task1\n",
    "target_locals = [\"DE\", 'JP', 'UK']\n",
    "\n",
    "# submit_file = f'submission_{task}_ALS.parquet'\n",
    "num_tree = 100\n",
    "w2v_model_dir = os.path.join(model_dir, w2v_model_version)\n",
    "w2v_model_file = os.path.join(w2v_model_dir, f\"{model_for_eval}.model\")\n",
    "annoy_index_file = os.path.join(w2v_model_dir, f\"{str(num_tree)}_{model_for_eval}.index\")\n",
    "\n",
    "\n",
    "\n",
    "train_dir = os.path.join(ranker_train_data_dir, 'train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "902bc312-8c20-42f7-ad41-b5d77d89fcce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "210036e4-72ad-4a2d-b704-5df89adede53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘../data/rank_train_data_v2’: File exists\n"
     ]
    }
   ],
   "source": [
    "! mkdir {ranker_train_data_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9f493af9-c566-4637-b9d9-a4371eb898dc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘../data/rank_train_data_v2/train’: File exists\n"
     ]
    }
   ],
   "source": [
    "! mkdir {train_dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3792fac-43cb-4754-9b10-9bcb1610dc0c",
   "metadata": {},
   "source": [
    "# Function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "195dd95c-2df7-4e10-b53e-34a07c51588b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def candidates_validation(target_pl):\n",
    "    # rec_summary  = (target_pl\n",
    "    #         .lazy()\n",
    "    #         .select(\n",
    "    #             pl.col('next_item_prediction').arr.lengths().alias('rec_num')\n",
    "    #         ).collect().describe())\n",
    "    # print(rec_summary)\n",
    "    max_sample_per_NIP = (\n",
    "        target_pl.groupby(['session_id', 'next_item_prediction'])\n",
    "            .agg(\n",
    "                pl.count().alias('unique_count')\n",
    "                # , pl.col('w2v_weight').max().alias('max_w2v_weight')\n",
    "                # , pl.col('w2v_weight').min().alias('min_w2v_weight')\n",
    "            )\n",
    "            .select(\n",
    "                pl.col('unique_count').max().alias('max_unique_cnt')\n",
    "                # , pl.col('max_w2v_weight').min().alias('min_max_w2v')\n",
    "                # , pl.col('min_w2v_weight').max().alias('max_min_w2v')\n",
    "            )\n",
    "    ).to_pandas()['max_unique_cnt'].values[0]\n",
    "    # print(stat_info)\n",
    "    # print(max_sample_per_NIP)\n",
    "    assert max_sample_per_NIP == 1, f\"there should be unique prediction for every session_id\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33cccd9d-18fd-4339-a739-39d7021cef73",
   "metadata": {},
   "source": [
    "# Original data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3486f554-4ca0-460c-9d31-4b4b1f2c5e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# eval_pl = pl.scan_parquet(os.path.join(base_dir, raw_data_session_id_dir, 'sessions_eval.parquet'), n_rows=SAMPLE_NUM).filter(pl.col('locale').is_in(target_locals)).with_columns(pl.col('prev_items').apply(str2list))\n",
    "\n",
    "# # df_sess.head(3).collect()\n",
    "# test_pl = pl.scan_parquet(os.path.join(base_dir, raw_data_session_id_dir, 'sessions_test_task1.parquet'), n_rows=SAMPLE_NUM).with_columns(pl.col('prev_items').apply(str2list))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef383a2-3ad1-4b70-b490-b607be419a8e",
   "metadata": {},
   "source": [
    "# Re-process data for memmory "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "56945b3b-dd62-49bf-b46f-89f98010deb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def re_process_data(task=task, \n",
    "                       w2v_model_version=w2v_model_version, \n",
    "                       nic_model_version=nic_model_version,\n",
    "                       nfi_model_version=nfi_model_version,\n",
    "                       covisit_model_version=covisit_model_version,\n",
    "                    model_for_eval=model_for_eval,\n",
    "                      w2v_topn=w2v_topn\n",
    "                      , nic_topn=nic_topn\n",
    "                       , nfi_topn=nfi_topn\n",
    "                       , covisit_topn=covisit_topn\n",
    "                    , train_parts=train_parts\n",
    "                      ):\n",
    "    parts = train_parts\n",
    "\n",
    "    data_type = 'train'\n",
    "    w2v_file = os.path.join(candidate_path, \n",
    "                           candidate_file_name.format(\n",
    "                    task=task\n",
    "                , data_type=data_type\n",
    "                , model_version=w2v_model_version\n",
    "                , model_for_eval=model_for_eval\n",
    "                , topn=w2v_topn\n",
    "                           ))\n",
    "    nic_file = os.path.join(candidate_path,\n",
    "                candidate_file_name.format(\n",
    "                    task=task\n",
    "                    , data_type=data_type\n",
    "                    , model_version=nic_model_version\n",
    "                    , model_for_eval=model_for_eval\n",
    "                    , topn=nic_topn\n",
    "                           ))\n",
    "    nfi_file = os.path.join(candidate_path,\n",
    "            candidate_file_name.format(\n",
    "                task=task\n",
    "                , data_type=data_type\n",
    "                , model_version=nfi_model_version\n",
    "                , model_for_eval=model_for_eval\n",
    "                , topn=nfi_topn\n",
    "                       ))\n",
    "    covisit_file = os.path.join(candidate_path,\n",
    "            candidate_file_name.format(\n",
    "                task=task\n",
    "                , data_type=data_type\n",
    "                , model_version=covisit_model_version\n",
    "                , model_for_eval=model_for_eval\n",
    "                , topn=covisit_topn\n",
    "                       ))\n",
    "    # w2v_pl = pl.scan_parquet(w2v_file, n_rows=SAMPLE_NUM).collect().sort('session_id')#.with_columns(pl.col('prev_items').apply(str2list))\n",
    "    # nic_pl = pl.scan_parquet(nic_file, n_rows=SAMPLE_NUM).collect().sort('session_id')#.with_columns(pl.col('prev_items').apply(str2list))\n",
    "    # nfi_pl = pl.scan_parquet(nfi_file, n_rows=SAMPLE_NUM).collect().sort('session_id')#.with_columns(pl.col('prev_items').apply(str2list))\n",
    "    # covisit_pl = pl.scan_parquet(covisit_file, n_rows=SAMPLE_NUM).collect().sort('session_id')#.with_columns(pl.col('prev_items').apply(str2list))\n",
    "    dir_lst = []\n",
    "    pl_lst = []\n",
    "    for file_name in [w2v_file, nic_file, nfi_file, covisit_file]:\n",
    "        data_pl = pl.scan_parquet(file_name, n_rows=SAMPLE_NUM).collect().sort('session_id')\n",
    "        dir_name = file_name.replace('.parquet', '')\n",
    "        print(dir_name)\n",
    "        dir_lst.append(dir_name)\n",
    "        pl_lst.append(data_pl)\n",
    "        if not os.path.isdir(dir_name):\n",
    "            os.mkdir(dir_name)\n",
    "    assert pl_lst[0].shape[0] == pl_lst[1].shape[0] == pl_lst[2].shape[0] == pl_lst[3].shape[0]\n",
    "    total_len = pl_lst[0].shape[0]\n",
    "\n",
    "    rows_per_part = int(total_len/parts)\n",
    "\n",
    "    for dir_path, data_pl in zip(dir_lst, pl_lst):\n",
    "        print(dir_path)\n",
    "        for idx in tqdm(range(parts)):\n",
    "            begin = idx*rows_per_part\n",
    "            if idx != parts-1:\n",
    "                end = (idx+1)*rows_per_part\n",
    "            else:\n",
    "                end = total_len\n",
    "            part_pl = data_pl[begin:end :]\n",
    "            file_name = os.path.join(dir_path, f\"part_{idx+1}.parquet\")\n",
    "            print(f'{begin} - {end} -> {file_name}')\n",
    "            part_pl.write_parquet(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3577175-b1b6-4f05-85e9-628e4a723e38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "706d01a1-1603-4e7c-b311-00a22985d25e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reprocess_train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d3b140a7-790d-4cfa-9664-761f05b93c74",
   "metadata": {},
   "outputs": [],
   "source": [
    "if reprocess_train_data:\n",
    "    re_process_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab84275-00e0-4a64-a668-c7eb3aa4e1c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "02a9baba-c2b7-441a-878f-0d71ad4e4dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! ls ../data/candidates/task1_train_w2v_v3_True_top100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "965a541a-6d56-4a0d-9470-7f1ae2a733e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pl.scan_parquet(os.path.join('../data/candidates/task1_train_co_visit_True_top300/', 'part_20.parquet'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a524368a-c385-4878-8042-60d248049b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.head().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e9d66b7-afd4-4207-a497-d733fe520055",
   "metadata": {},
   "source": [
    "# Get candiadtes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "655df00b-b421-41ff-a016-52ec598e2120",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_type = 'eval'\n",
    "# w2v_file = os.path.join(candidate_path, \n",
    "#                            candidate_file_name.format(\n",
    "#                     task=task\n",
    "#                 , data_type=data_type\n",
    "#                 , model_version=w2v_model_version\n",
    "#                 , model_for_eval=model_for_eval\n",
    "#                 , topn=w2v_topn\n",
    "#                            ))\n",
    "# w2v_file\n",
    "# w2v_pl = pl.scan_parquet(w2v_file, n_rows=SAMPLE_NUM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ae5d05e1-c0dc-428f-b083-162ef74b4a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def get_all_candidates(data_type, task=task, \n",
    "                       w2v_model_version=w2v_model_version, \n",
    "                       nic_model_version=nic_model_version,\n",
    "                       nfi_model_version=nfi_model_version,\n",
    "                       covisit_model_version=covisit_model_version,\n",
    "                    model_for_eval=model_for_eval,\n",
    "                      w2v_topn=w2v_topn\n",
    "                      , nic_topn=nic_topn\n",
    "                       , nfi_topn=nfi_topn\n",
    "                       , covisit_topn=covisit_topn\n",
    "                       , part=None\n",
    "                      ):\n",
    "    w2v_file = os.path.join(candidate_path, \n",
    "                           candidate_file_name.format(\n",
    "                    task=task\n",
    "                , data_type=data_type\n",
    "                , model_version=w2v_model_version\n",
    "                , model_for_eval=model_for_eval\n",
    "                , topn=w2v_topn\n",
    "                           ))\n",
    "    nic_file = os.path.join(candidate_path,\n",
    "                candidate_file_name.format(\n",
    "                    task=task\n",
    "                    , data_type=data_type\n",
    "                    , model_version=nic_model_version\n",
    "                    , model_for_eval=model_for_eval\n",
    "                    , topn=nic_topn\n",
    "                           ))\n",
    "    nfi_file = os.path.join(candidate_path,\n",
    "            candidate_file_name.format(\n",
    "                task=task\n",
    "                , data_type=data_type\n",
    "                , model_version=nfi_model_version\n",
    "                , model_for_eval=model_for_eval\n",
    "                , topn=nfi_topn\n",
    "                       ))\n",
    "    covisit_file = os.path.join(candidate_path,\n",
    "            candidate_file_name.format(\n",
    "                task=task\n",
    "                , data_type=data_type\n",
    "                , model_version=covisit_model_version\n",
    "                , model_for_eval=model_for_eval\n",
    "                , topn=covisit_topn\n",
    "                       ))\n",
    "    print(\"Candidiate files: \")\n",
    "    print('+'*20)\n",
    "    print()\n",
    "    print(w2v_file)\n",
    "    print(nic_file)\n",
    "    print(nfi_file)\n",
    "    print(covisit_file)\n",
    "    if data_type == 'train':\n",
    "        assert part is not None\n",
    "        w2v_file = os.path.join(w2v_file.replace('.parquet', ''), f\"part_{part}.parquet\")\n",
    "        nic_file = os.path.join(nic_file.replace('.parquet', ''), f\"part_{part}.parquet\")\n",
    "        nfi_file = os.path.join(nfi_file.replace('.parquet', ''), f\"part_{part}.parquet\")\n",
    "        covisit_file = os.path.join(covisit_file.replace('.parquet', ''), f\"part_{part}.parquet\")\n",
    "\n",
    "    if data_type == 'test':\n",
    "        original_file_name = f\"sessions_{data_type}_{task}.parquet\"\n",
    "    if data_type == 'test4task3':\n",
    "        original_file_name = f\"sessions_test_task3.parquet\"\n",
    "    if data_type == 'eval':\n",
    "        original_file_name = f\"sessions_{data_type}.parquet\"\n",
    "    original_pl = pl.scan_parquet(os.path.join(base_dir, raw_data_session_id_dir, original_file_name), n_rows=SAMPLE_NUM).filter(pl.col('locale').is_in(target_locals)).with_columns(pl.col('prev_items').apply(str2list))\n",
    "    w2v_pl = pl.scan_parquet(w2v_file, n_rows=SAMPLE_NUM)#.with_columns(pl.col('prev_items').apply(str2list))\n",
    "    nic_pl = pl.scan_parquet(nic_file, n_rows=SAMPLE_NUM)#.with_columns(pl.col('prev_items').apply(str2list))\n",
    "    nfi_pl = pl.scan_parquet(nfi_file, n_rows=SAMPLE_NUM)#.with_columns(pl.col('prev_items').apply(str2list))\n",
    "    covisit_pl = pl.scan_parquet(covisit_file, n_rows=SAMPLE_NUM)#.with_columns(pl.col('prev_items').apply(str2list))\n",
    "\n",
    "\n",
    "#     def get_vector(prev_items):\n",
    "#         prev_items_vec = np.array([w2vec.wv[ele] for ele in prev_items]).mean(axis=0)\n",
    "#         return prev_items_vec\n",
    "    \n",
    "#     original_pl = (\n",
    "#             original_pl.with_columns(\n",
    "#                 pl.col('prev_items').apply(lambda x: get_vector(x)).alias('all_previous_items_vec')\n",
    "#             )\n",
    "#         )\n",
    "    \n",
    "    # get w2v weight\n",
    "    w2v_pl = w2v_pl.with_columns(pl.col('next_item_prediction').arr.lengths().alias('rec_num')).with_columns(\n",
    "        pl.col('rec_num').apply(lambda x: list(range(x, 0, -1))).alias('w2v_weight')\n",
    "        \n",
    "    )\n",
    "    # print(w2v_pl.head().collect())\n",
    "    get_w2v_weight = pl.element().rank()*0\n",
    "    nic_pl = nic_pl.with_columns(\n",
    "        pl.col('next_item_prediction').arr.eval(get_w2v_weight, \n",
    "                                                # parallel=True\n",
    "                                               ).alias('w2v_weight').cast(pl.List(pl.Int64))\n",
    "    )\n",
    "    nfi_pl = nfi_pl.with_columns(\n",
    "        pl.col('next_item_prediction').arr.eval(get_w2v_weight, \n",
    "                                               ).alias('w2v_weight').cast(pl.List(pl.Int64))\n",
    "    )\n",
    "    covisit_pl = covisit_pl.with_columns(\n",
    "        pl.col('next_item_prediction').arr.eval(get_w2v_weight, \n",
    "                                               ).alias('w2v_weight').cast(pl.List(pl.Int64))\n",
    "    )\n",
    "    cols = ['session_id', 'next_item_prediction', \n",
    "            'w2v_weight'\n",
    "           ]\n",
    "    # combined_pl = (\n",
    "    #     w2v_pl.select(cols)\n",
    "    #         .join(nic_pl.select(cols), how='left', on='session_id', suffix='_nic')\n",
    "    #         .join(nfi_pl.select(cols), how='left', on='session_id', suffix='_nfi')\n",
    "    # )\n",
    "\n",
    "    # combined_pl = combined_pl.with_columns(\n",
    "    #     pl.concat_list([pl.col('next_item_prediction'), pl.col('next_item_prediction_nic')]).alias('next_item_prediction'), \n",
    "    #     pl.concat_list([pl.col('w2v_weight'), pl.col('w2v_weight_nic')]).alias('w2v_weight'), \n",
    "    # ).select(\n",
    "    #     pl.all().exclude(['next_item_prediction_nic', 'w2v_weight_nic'])\n",
    "    # )\n",
    "    explode_cols = ['next_item_prediction', \n",
    "                    'w2v_weight'\n",
    "                   ]\n",
    "    combined_pl = (\n",
    "            pl.concat([w2v_pl.select(cols).explode(explode_cols)\n",
    "                       , nic_pl.select(cols).explode(explode_cols)\n",
    "                       , nfi_pl.select(cols).explode(explode_cols)\n",
    "                       , covisit_pl.select(cols).explode(explode_cols)\n",
    "                      ], how='vertical')\n",
    "                .groupby(['session_id', 'next_item_prediction'])\n",
    "                .agg(\n",
    "                    pl.col('w2v_weight').max().alias('w2v_weight')\n",
    "                )\n",
    "                .join(original_pl.select(['session_id', 'prev_items']), how='left', on='session_id')\n",
    "                .filter(pl.col('prev_items').arr.contains(pl.col('next_item_prediction'))==False)\n",
    "                .with_columns(\n",
    "                    pl.col('w2v_weight').max().over('session_id').alias('max_count')\n",
    "                    , pl.col('w2v_weight').min().over('session_id').alias('min_count')\n",
    "                )\n",
    "                .with_columns(\n",
    "                    pl.when(pl.col('max_count')==pl.col('min_count')).then(1).otherwise((pl.col('w2v_weight')-pl.col('min_count'))/(pl.col('max_count')-pl.col('min_count'))).alias('w2v_weight')\n",
    "                )\n",
    "                .select(\n",
    "                        pl.exclude(['prev_items', 'max_count', 'min_count'])\n",
    "\n",
    "                )\n",
    "                .groupby('session_id')\n",
    "                .agg(\n",
    "                    pl.all()\n",
    "                )\n",
    "    )\n",
    "    combined_pl = (\n",
    "        combined_pl.join(original_pl, how='left', on='session_id')\n",
    "            .with_columns(\n",
    "                pl.col('prev_items').arr.lengths().alias('prev_length')\n",
    "                , pl.col('prev_items').arr.get(-1).alias('last_prev_item')\n",
    "            ).select(\n",
    "                pl.all().exclude('prev_items')\n",
    "            )\n",
    "    )\n",
    "    return combined_pl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cace44d1-7a16-4e49-90e1-7962134104fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f6493505-cef7-4630-9c3a-0b436d2f9189",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # w2v_pl = pl.scan_parquet(w2v_file, n_rows=SAMPLE_NUM)#.with_columns(pl.col('prev_items').apply(str2list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5242ea97-e5dd-45aa-96ef-043d295133fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5450fc2a-d9ee-4895-b972-0033129c3193",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Candidiate files: \n",
      "++++++++++++++++++++\n",
      "\n",
      "../data/candidates/task1_eval_w2v_v3_True_top200.parquet\n",
      "../data/candidates/task1_eval_nic_True_top100.parquet\n",
      "../data/candidates/task1_eval_next_few_items_v1_True_top100.parquet\n",
      "../data/candidates/task1_eval_co_visit_True_top300.parquet\n"
     ]
    }
   ],
   "source": [
    "eval_cg_pl = get_all_candidates(data_type='eval')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a90f2fe7-29c4-4ac4-ab33-8c8569092fa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task1_test4task3_co_visit_True_top300.parquet\n",
      "task1_test4task3_next_few_items_v1_True_top100.parquet\n",
      "task1_test4task3_next_item_counter_v2_True_top100.parquet\n",
      "task1_test_co_visit_True_top300.parquet\n",
      "task1_test_co_visit_True_top300_phase1.parquet\n",
      "task1_test_next_few_items_v1_True_top100.parquet\n",
      "task1_test_next_few_items_v1_True_top100_phase1.parquet\n",
      "task1_test_nic_True_top100.parquet\n",
      "task1_test_nic_True_top100_phase1.parquet\n",
      "task1_test_w2v_v3_True_top100_phase1.parquet\n",
      "task1_test_w2v_v3_True_top200.parquet\n",
      "task2_test4task3_co_visit_task2_True_top300.parquet\n",
      "task2_test4task3_nfi_task2_True_top100.parquet\n",
      "task2_test4task3_nic_task2_True_top100.parquet\n",
      "task2_test4task3_w2v_task2_True_top200.parquet\n",
      "task2_test_co_visit_task2_True_top300.parquet\n",
      "task2_test_nfi_task2_True_top100.parquet\n",
      "task2_test_nic_True_top100.parquet\n",
      "task2_test_nic_task2_True_top100.parquet\n",
      "task2_test_w2v_task2_True_top200.parquet\n"
     ]
    }
   ],
   "source": [
    "! ls ../data/candidates/ | grep test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "98174218-9b34-4b88-9ad3-2a84bcbe28cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Candidiate files: \n",
      "++++++++++++++++++++\n",
      "\n",
      "../data/candidates/task1_test_w2v_v3_True_top200.parquet\n",
      "../data/candidates/task1_test_nic_True_top100.parquet\n",
      "../data/candidates/task1_test_next_few_items_v1_True_top100.parquet\n",
      "../data/candidates/task1_test_co_visit_True_top300.parquet\n"
     ]
    }
   ],
   "source": [
    "test_cg_pl = get_all_candidates(data_type='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "833a63a6-4528-42cc-8617-fa01cb735999",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval_cg_pl.head().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ba652df1-cb57-4e41-802f-cb82cd4bfbbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval_cg_pl.select(\n",
    "#     pl.col('next_item_prediction').arr.lengths().alias('len1')\n",
    "#     , pl.col('w2v_weight').arr.lengths().alias('len2')\n",
    "# ).collect().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d2ca92ac-7ec5-49f2-8f10-d0525f940b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval_cg_pl.select(\n",
    "#     pl.col('next_item_prediction').arr.lengths().alias('len1')\n",
    "#     , pl.col('w2v_weight').arr.lengths().alias('len2')\n",
    "# ).collect().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "83fceb6d-9b95-43a4-a6a4-8d85ad28362b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ).collect().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b40aeaa9-deae-4fcc-9a61-ebb95d1eef15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval_cg_pl.head().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b9d1ea67-ba17-4306-95fd-117132714fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval_cg_pl.head().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db6d45e-36d7-4844-a312-e419b7d9ff3e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Eval candidate generation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3bcff578-861e-46af-b1f7-fab9d7a774af",
   "metadata": {},
   "outputs": [],
   "source": [
    "if eval_cg:\n",
    "    eval_topn = 3000\n",
    "\n",
    "    col = f\"recall@{eval_topn}\"\n",
    "    eval_final = (\n",
    "            eval_cg_pl\n",
    "            .lazy()\n",
    "            .with_columns(\n",
    "                pl.col('next_item_prediction').cast(pl.List(pl.Utf8))\n",
    "            )\n",
    "            ).with_columns(\n",
    "                pl.col('next_item_prediction').arr.head(eval_topn).arr.contains(pl.col('next_item')).mean().alias(col)\n",
    "\n",
    "            )\n",
    "    final_res = eval_final.select(\n",
    "            pl.count().alias('total_sessions')\n",
    "            , pl.col(col).mean()\n",
    "\n",
    "        ).collect()\n",
    "    describe = (eval_cg_pl\n",
    "            .lazy()\n",
    "            .select(\n",
    "                pl.col('next_item_prediction').arr.lengths().alias('rec_num')\n",
    "            ).collect().describe())\n",
    "    print(describe)\n",
    "    print(final_res)\n",
    "    del final_res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a40c22f-010f-4bba-ae1f-aa76247c638d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Feature Process "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1552490e-efa5-49fe-810f-954bdc3f6517",
   "metadata": {},
   "source": [
    "## Load NIC, W2V Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a9c64bc6-0657-455d-bf4c-f4cfe23d6e1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'next_item_prediction': Utf8, 'next_item_weight': Float64, 'last_prev_item': Utf8}\n",
      "{'next_item_prediction': Utf8, 'next_item_weight': Float32, 'last_prev_item': Utf8}\n"
     ]
    }
   ],
   "source": [
    "nic_model = (\n",
    "    pl.scan_parquet('../model_training/next_item_counter_v2/nic_model.parquet')\n",
    "        .explode(['next_item_prediction', 'next_item_weight'])\n",
    "        .select(\n",
    "            pl.all().exclude('item')\n",
    "            , pl.col('item').alias('last_prev_item')\n",
    "        )\n",
    "            )\n",
    "print(nic_model.schema)\n",
    "\n",
    "nfi_model = (\n",
    "    pl.scan_parquet('../model_training/next_few_items_v1/nic_True_for_eval.parquet')\n",
    "        .explode(['next_item_prediction', 'next_item_weight'])\n",
    "        .select(\n",
    "            pl.all().exclude('item')\n",
    "            , pl.col('item').alias('last_prev_item')\n",
    "            # , pl.col()\n",
    "        )\n",
    "            )\n",
    "print(nfi_model.schema)\n",
    "\n",
    "\n",
    "\n",
    "# w2v_model_file = '../model_training/v2/w2v.model'\n",
    "w2vec = Word2Vec.load(w2v_model_file)\n",
    "annoy_index = AnnoyIndexer()\n",
    "annoy_index.load(annoy_index_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ae5c4071-d995-4be6-b98d-7c189896d558",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'next_item_prediction': Utf8, 'next_item_weight': Float32, 'last_prev_item': Utf8}\n"
     ]
    }
   ],
   "source": [
    "co_visit_model = (\n",
    "    pl.scan_parquet(f'../model_training/{covisit_model_version}/{covisit_model_version}_True_for_eval.parquet')\n",
    "        .explode(['next_item_prediction', 'next_item_weight'])\n",
    "        .select(\n",
    "            pl.all().exclude('item')\n",
    "            , pl.col('item').alias('last_prev_item')\n",
    "            # , pl.col()\n",
    "        )\n",
    "            )\n",
    "print(co_visit_model.schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7ec1bbd1-d958-4958-854c-eea616c49118",
   "metadata": {},
   "outputs": [],
   "source": [
    "# co_visit_model.head().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7ccd61-b973-4497-9afe-0c84c7f7af9b",
   "metadata": {},
   "source": [
    "## get features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9bd3e83e-001e-4634-a12b-fdfa3e2b8b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# target_df = train_candidates\n",
    "# data_type = 'train'\n",
    "\n",
    "def get_w2v_simi(x, col='last_prev_item'):\n",
    "    try:\n",
    "        if isinstance(x[col], str):\n",
    "            target = [x[col]]\n",
    "        else:\n",
    "            target = x[col]\n",
    "        simi = w2vec.wv.n_similarity([x['next_item_prediction']],\n",
    "                                                  target\n",
    "                                                 )\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        # print(x[col])\n",
    "        simi = 0\n",
    "    return simi\n",
    "\n",
    "def get_feature(target_df, data_type, nic_model=nic_model, nfi_model=nfi_model, \n",
    "                co_visit_model=co_visit_model,\n",
    "                negative_sampling=None, w2v_similarity=False):\n",
    "    if data_type == 'train':\n",
    "        target_df = target_df.filter(pl.col('next_item_prediction').arr.contains(pl.col('next_item')))\n",
    "    target_df = target_df.explode(['next_item_prediction', 'w2v_weight'])\n",
    "\n",
    "    if data_type not in ['test', 'test4task3']:\n",
    "        target_df = (\n",
    "            target_df\n",
    "                .with_columns(\n",
    "                    pl.when(pl.col('next_item')==pl.col('next_item_prediction')).then(1).otherwise(0).alias('target')\n",
    "                )\n",
    "        )\n",
    "\n",
    "    if negative_sampling is not None:\n",
    "        cols = ['session_id', 'next_item', 'next_item_prediction', 'target']\n",
    "        sample_pl = target_df.select(cols)\n",
    "        pos = sample_pl.filter(pl.col('target')==1)\n",
    "        neg = (sample_pl\n",
    "                   .filter(pl.col('target')==0)\n",
    "              )\n",
    "        neg_original_len = neg.select('session_id').collect().shape[0]\n",
    "        print(f'Original negative num: {neg_original_len}')\n",
    "        # neg = (\n",
    "        #     neg.with_columns(\n",
    "        #                 pl.Series(name='random', values=np.random.uniform(size=neg_original_len))\n",
    "        #            )\n",
    "        #            .filter(pl.col('random')<=negative_sampling)\n",
    "        #            .select(pl.all().exclude('random'))\n",
    "        # )\n",
    "        \n",
    "        neg = neg.groupby('session_id').agg(\n",
    "            pl.all().sample(frac=negative_sampling)\n",
    "            )\n",
    "        neg = neg.explode(neg.columns[1:])\n",
    "        # # print(neg.shape)\n",
    "        # # print(neg.head().collect())\n",
    "        sample_pl = pl.concat([pos, neg], how='vertical')\n",
    "        target_df = sample_pl.join(target_df, how='left', on=cols)#.collect()\n",
    "        del sample_pl\n",
    "    target_df = (\n",
    "            target_df.lazy().join(nic_model, how='left', on=['last_prev_item', 'next_item_prediction'])\n",
    "                .join(nfi_model, how='left', on=['last_prev_item', 'next_item_prediction'], suffix='_nfi')\n",
    "                .join(co_visit_model, how='left', on=['last_prev_item', 'next_item_prediction'], suffix='_co_visit')\n",
    "                .with_columns(\n",
    "                    pl.when(pl.col('next_item_weight').is_null()).then(0).otherwise(pl.col('next_item_weight')).alias('next_item_weight')\n",
    "                    , pl.when(pl.col('next_item_weight_nfi').is_null()).then(0).otherwise(pl.col('next_item_weight_nfi')).alias('next_item_weight_nfi')\n",
    "                    , pl.when(pl.col('next_item_weight_co_visit').is_null()).then(0).otherwise(pl.col('next_item_weight_co_visit')).alias('next_item_weight_co_visit')\n",
    "                    , pl.struct([\"next_item_prediction\", \"last_prev_item\"]).apply(\n",
    "                        lambda x: get_w2v_simi(x, col='last_prev_item')).alias('last_item_similarity').cast(pl.Float32)\n",
    "                    # , pl.struct([\"next_item_prediction\", \"prev_items\"]).apply(\n",
    "                    #     lambda x: get_w2v_simi(x, col='prev_items')).alias('prev_item_similarity').cast(pl.Float32)\n",
    "                    , pl.when(pl.col('locale')=='DE').then(1).when(pl.col('locale')=='DE')\n",
    "                        .then(2)\n",
    "                        .otherwise(3).alias('locale')\n",
    "                ).sort('session_id')\n",
    "                .select(\n",
    "                    pl.all().exclude('last_prev_item')\n",
    "                )\n",
    "\n",
    "    )\n",
    "    # if w2v_similarity:\n",
    "    #     target_df = target_df.with_columns(\n",
    "    #         pl.struct([\"next_item_prediction\", \"prev_items\"]).apply(\n",
    "    #                     lambda x: get_w2v_simi(x, col='prev_items')).alias('prev_item_similarity').cast(pl.Float32)\n",
    "    #     )\n",
    "    return target_df\n",
    "# target_df.head(3).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "be32562e-3c44-4aa9-a119-39d78665bf40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time \n",
    "# sample_pl = get_feature(target_df=train_cg_pl, data_type='train', #negative_sampling=0.1\n",
    "#                        ).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b090d4b4-7af7-4095-ab64-0f83bd8c9850",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sampled_train_candidates.head().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5a9a8901-c9e9-4910-a5f1-bef4b6e07894",
   "metadata": {},
   "outputs": [],
   "source": [
    "# part_index = 0\n",
    "# print()\n",
    "# file_name = os.path.join(train_dir, f\"part_{part_index+1}.parquet\")\n",
    "# print(file_name)\n",
    "# train_cg_pl = get_all_candidates(data_type='train', part=part_index+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9f4bc5b0-e5a5-45e5-87b5-a93c33eb4214",
   "metadata": {},
   "outputs": [],
   "source": [
    "# help(pl.all().sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a3c70107-60a7-410b-bf2f-c3ebde6de2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# sampled_train_candidates = get_feature(target_df=train_cg_pl, data_type='train', negative_sampling=0.1)\n",
    "# # debug_pl = sampled_train_candidates.head().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f9578598-00a8-4233-a8ef-d288f27c810f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# stat_info = (\n",
    "#         sampled_train_candidates\n",
    "#             .groupby('session_id')\n",
    "#             .agg(\n",
    "#                 pl.count().alias('rec_cnt')\n",
    "#                 , pl.col('target').sum().alias('target_num')\n",
    "#             )\n",
    "#             .collect()\n",
    "#             .describe()\n",
    "#     )\n",
    "# print(stat_info)\n",
    "# (\n",
    "#         train_cg_pl.select(\n",
    "#                 pl.col('next_item_prediction').arr.lengths().alias('rec_num')\n",
    "#             )\n",
    "#             .collect()\n",
    "#             .describe(\n",
    "#                 # percentiles=[0.1*i for i in range(10)]\n",
    "#             )\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "11929217-d917-4a12-ac67-03de86b700d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# stat_info = (\n",
    "#         sampled_train_candidates\n",
    "#             .groupby('session_id')\n",
    "#             .agg(\n",
    "#                 pl.count().alias('rec_cnt')\n",
    "#                 , pl.col('target').sum().alias('target_num')\n",
    "#             )\n",
    "#             .collect()\n",
    "#             .describe()\n",
    "#     )\n",
    "# print(stat_info)\n",
    "# (\n",
    "#         train_cg_pl.select(\n",
    "#                 pl.col('next_item_prediction').arr.lengths().alias('rec_num')\n",
    "#             )\n",
    "#             .collect()\n",
    "#             .describe(\n",
    "#                 # percentiles=[0.1*i for i in range(10)]\n",
    "#             )\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92bb8763-c567-4bbb-82d2-7883396d2dcd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39e7bb2-4473-4295-b8da-6396ed147500",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef78521-ebc4-4138-ad4e-860150aaa7a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "aab35a1f-12d6-4a32-b04d-32cd9a970900",
   "metadata": {},
   "outputs": [],
   "source": [
    "# debug_pl.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8bec2c8c-8f7a-4fa3-a7b0-3deae4240837",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# sampled_train_candidates = get_feature(target_df=train_cg_pl, data_type='train',\n",
    "#                                        negative_sampling=0.1,\n",
    "#                                       w2v_similarity=True)\n",
    "# debug_pl = sampled_train_candidates.head().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "887d90b4-d9d1-471c-85e9-32d754647349",
   "metadata": {},
   "outputs": [],
   "source": [
    "# debug_pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ca0b2625-e8d3-4feb-81e6-6f8d81344934",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prev_items = [\"B09W9FND7K\", \"B09JSPLN1M\"]\n",
    "# next_item_prediction = \"B09M7GY217\"\t\n",
    "# w2vec.wv.n_similarity(prev_items, [next_item_prediction])\n",
    "# w2vec.wv.n_similarity(prev_items, prev_items)\n",
    "# prev_items_vec = np.array([w2vec.wv[ele] for ele in prev_items]).mean(axis=0)\n",
    "# next_item_vec = w2vec.wv[next_item_prediction]\n",
    "# 1 - scipy.spatial.distance.cosine(prev_items_vec, next_item_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4e8547-6eb6-4533-83c8-b22a17f34803",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Save Data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09d68c1-49c2-42b8-acfc-eb05fc45f75d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Save eval data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "46dab88f-e151-4fcb-9084-a354a3d48965",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext memory_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "85ea0e28-9394-488d-b332-a403a74cfc65",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_candidates = get_feature(target_df=eval_cg_pl, data_type='eval')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "33fd9bae-f954-453f-a185-e09c0c199e15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 1626.96 MiB, increment: 0.10 MiB\n",
      "Eval data\n",
      "\n",
      "(64153741, 11)\n",
      "Memory: 5.033693803474307\n",
      "0.0031155782481960014\n",
      "Validating\n",
      "\n",
      "CPU times: user 56min 36s, sys: 3min 5s, total: 59min 42s\n",
      "Wall time: 49min 1s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%memit\n",
    "eval_candidates = eval_candidates.collect()\n",
    "\n",
    "print('Eval data')\n",
    "print()\n",
    "print(eval_candidates.shape)\n",
    "print(f\"Memory: {eval_candidates.estimated_size(unit='gb')}\")\n",
    "print(eval_candidates['target'].mean())\n",
    "print('Validating')\n",
    "candidates_validation(target_pl=eval_candidates)\n",
    "print()\n",
    "\n",
    "if not debug:\n",
    "    eval_candidates.write_parquet(os.path.join(ranker_train_data_dir, 'eval.parquet'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3516dd0f-f90b-46f9-8948-a49a0a651915",
   "metadata": {},
   "source": [
    "## test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e4a2c7d1-31ba-412c-9acd-fef85ba3cfdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_candidates = get_feature(target_df=test_cg_pl, data_type='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "25e68e10-b3c4-43dd-8a1c-b746285dc756",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext memory_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a54c6756-d45a-4dee-8f01-21eeefb5ffe3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../data/rank_train_data_v2'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ranker_train_data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee0c22d-af0f-47be-9338-5e58eaa22654",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 1052.30 MiB, increment: 0.01 MiB\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%memit\n",
    "\n",
    "test_candidates = test_candidates.collect()\n",
    "print('Test data')\n",
    "print()\n",
    "print(test_candidates.shape)\n",
    "print(f\"Memory: {test_candidates.estimated_size(unit='gb')}\")\n",
    "print('Validating')\n",
    "candidates_validation(target_pl=test_candidates)\n",
    "\n",
    "if not debug:\n",
    "    # eval_candidates.write_parquet(os.path.join(ranker_train_data_dir, 'eval.parquet'))\n",
    "    test_candidates.write_parquet(os.path.join(ranker_train_data_dir, 'test.parquet'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e0b079e4-1309-4675-9656-607282a50099",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94bb5824-18bf-4c59-839e-a054f00585c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08aa8b39-fef3-4aa3-8dfb-ec4ce98d13fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval_candidates.write_parquet(os.path.join(ranker_train_data_dir, 'eval.parquet'))\n",
    "# test_candidates.write_parquet(os.path.join(ranker_train_data_dir, 'test.parquet'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8459e6d0-012c-4856-b338-5cb6b743562b",
   "metadata": {},
   "source": [
    "### test4task3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a1ef7f4c-7475-46e8-a8e9-d071a11779ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task1_test4task3_co_visit_True_top300.parquet\n",
      "task1_test4task3_next_few_items_v1_True_top100.parquet\n",
      "task1_test4task3_next_item_counter_v2_True_top100.parquet\n",
      "task1_test4task3_w2v_v3_True_top200.parquet\n",
      "task2_test4task3_co_visit_task2_True_top300.parquet\n",
      "task2_test4task3_nfi_task2_True_top100.parquet\n",
      "task2_test4task3_nic_task2_True_top100.parquet\n",
      "task2_test4task3_w2v_task2_True_top200.parquet\n"
     ]
    }
   ],
   "source": [
    "! ls ../data/candidates/ | grep test4task3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4b0da582-16da-46a8-9fd3-f3d9be0ac78a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Candidiate files: \n",
      "++++++++++++++++++++\n",
      "\n",
      "../data/candidates/task1_test4task3_w2v_v3_True_top200.parquet\n",
      "../data/candidates/task1_test4task3_nic_True_top100.parquet\n",
      "../data/candidates/task1_test4task3_next_few_items_v1_True_top100.parquet\n",
      "../data/candidates/task1_test4task3_co_visit_True_top300.parquet\n"
     ]
    }
   ],
   "source": [
    "test4task3_cg_pl = get_all_candidates(data_type='test4task3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ace691a5-773e-4e73-8941-b9e891325ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# '../data/candidates/task1_test4task3_w2v_v3_True_top200.parquet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4da396b2-ded3-4b4b-9b5e-a73c9abd5c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# task1_test_w2v_v3_True_top100_phase1.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "44103b36-ab2a-4e5a-be3c-6d47380dea84",
   "metadata": {},
   "outputs": [],
   "source": [
    "test4task3_candidates = get_feature(target_df=test4task3_cg_pl, data_type='test4task3')\n",
    "test4task3_candidates.collect().write_parquet(os.path.join(ranker_train_data_dir, 'test4task3.parquet'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6682bdad-922c-4928-ac8b-5f296f123f80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task1_test4task3_co_visit_True_top300.parquet\n",
      "task1_test4task3_next_few_items_v1_True_top100.parquet\n",
      "task1_test4task3_nic_True_top100.parquet\n",
      "task1_test4task3_w2v_v3_True_top200.parquet\n",
      "task2_test4task3_co_visit_task2_True_top300.parquet\n",
      "task2_test4task3_nfi_task2_True_top100.parquet\n",
      "task2_test4task3_nic_task2_True_top100.parquet\n",
      "task2_test4task3_w2v_task2_True_top200.parquet\n"
     ]
    }
   ],
   "source": [
    "! ls ../data/candidates/ | grep test4task3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53711885-34f0-4961-a44e-2978a76db956",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Saving train data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1c94e2-674e-4c8d-b2f5-21c92ba4690d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_cg_pl.collect().sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d7c43d-db32-4997-a2f3-dc87c863afc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "../data/rank_train_data_v2/train/part_1.parquet\n",
      "Original negative num: 28419506\n",
      "Validating\n",
      "(2923449, 11)\n",
      "Memory: 0.22870463505387306\n",
      "0.05013427632908937\n",
      "shape: (7, 4)\n",
      "┌────────────┬──────────────┬───────────┬────────────┐\n",
      "│ describe   ┆ session_id   ┆ rec_cnt   ┆ target_num │\n",
      "│ ---        ┆ ---          ┆ ---       ┆ ---        │\n",
      "│ str        ┆ f64          ┆ f64       ┆ f64        │\n",
      "╞════════════╪══════════════╪═══════════╪════════════╡\n",
      "│ count      ┆ 146565.0     ┆ 146565.0  ┆ 146565.0   │\n",
      "│ null_count ┆ 0.0          ┆ 0.0       ┆ 0.0        │\n",
      "│ mean       ┆ 81754.567209 ┆ 19.946433 ┆ 1.0        │\n",
      "│ std        ┆ 47232.319874 ┆ 9.651442  ┆ 0.0        │\n",
      "│ min        ┆ 0.0          ┆ 8.0       ┆ 1.0        │\n",
      "│ max        ┆ 163580.0     ┆ 47.0      ┆ 1.0        │\n",
      "│ median     ┆ 81763.0      ┆ 16.0      ┆ 1.0        │\n",
      "└────────────┴──────────────┴───────────┴────────────┘\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 1/20 [06:13<1:58:17, 373.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "../data/rank_train_data_v2/train/part_2.parquet\n",
      "Original negative num: 28439739\n",
      "Validating\n",
      "(2925578, 11)\n",
      "Memory: 0.228871189057827\n",
      "0.050086512819005335\n",
      "shape: (7, 4)\n",
      "┌────────────┬───────────────┬───────────┬────────────┐\n",
      "│ describe   ┆ session_id    ┆ rec_cnt   ┆ target_num │\n",
      "│ ---        ┆ ---           ┆ ---       ┆ ---        │\n",
      "│ str        ┆ f64           ┆ f64       ┆ f64        │\n",
      "╞════════════╪═══════════════╪═══════════╪════════════╡\n",
      "│ count      ┆ 146532.0      ┆ 146532.0  ┆ 146532.0   │\n",
      "│ null_count ┆ 0.0           ┆ 0.0       ┆ 0.0        │\n",
      "│ mean       ┆ 245370.960937 ┆ 19.965455 ┆ 1.0        │\n",
      "│ std        ┆ 47185.387385  ┆ 9.643766  ┆ 0.0        │\n",
      "│ min        ┆ 163583.0      ┆ 8.0       ┆ 1.0        │\n",
      "│ max        ┆ 327062.0      ┆ 47.0      ┆ 1.0        │\n",
      "│ median     ┆ 245434.5      ┆ 16.0      ┆ 1.0        │\n",
      "└────────────┴───────────────┴───────────┴────────────┘\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 2/20 [12:39<1:54:19, 381.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "../data/rank_train_data_v2/train/part_3.parquet\n",
      "Original negative num: 28388961\n",
      "Validating\n",
      "(2920473, 11)\n",
      "Memory: 0.2284718193113804\n",
      "0.05018091247547914\n",
      "shape: (7, 4)\n",
      "┌────────────┬───────────────┬───────────┬────────────┐\n",
      "│ describe   ┆ session_id    ┆ rec_cnt   ┆ target_num │\n",
      "│ ---        ┆ ---           ┆ ---       ┆ ---        │\n",
      "│ str        ┆ f64           ┆ f64       ┆ f64        │\n",
      "╞════════════╪═══════════════╪═══════════╪════════════╡\n",
      "│ count      ┆ 146552.0      ┆ 146552.0  ┆ 146552.0   │\n",
      "│ null_count ┆ 0.0           ┆ 0.0       ┆ 0.0        │\n",
      "│ mean       ┆ 408759.735111 ┆ 19.927896 ┆ 1.0        │\n",
      "│ std        ┆ 47202.53323   ┆ 9.639351  ┆ 0.0        │\n",
      "│ min        ┆ 327063.0      ┆ 8.0       ┆ 1.0        │\n",
      "│ max        ┆ 490553.0      ┆ 47.0      ┆ 1.0        │\n",
      "│ median     ┆ 408829.5      ┆ 16.0      ┆ 1.0        │\n",
      "└────────────┴───────────────┴───────────┴────────────┘\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 3/20 [19:07<1:48:49, 384.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "../data/rank_train_data_v2/train/part_4.parquet\n",
      "Original negative num: 28418720\n",
      "Validating\n",
      "(2923489, 11)\n",
      "Memory: 0.22870776429772377\n",
      "0.05014624648835689\n",
      "shape: (7, 4)\n",
      "┌────────────┬───────────────┬───────────┬────────────┐\n",
      "│ describe   ┆ session_id    ┆ rec_cnt   ┆ target_num │\n",
      "│ ---        ┆ ---           ┆ ---       ┆ ---        │\n",
      "│ str        ┆ f64           ┆ f64       ┆ f64        │\n",
      "╞════════════╪═══════════════╪═══════════╪════════════╡\n",
      "│ count      ┆ 146602.0      ┆ 146602.0  ┆ 146602.0   │\n",
      "│ null_count ┆ 0.0           ┆ 0.0       ┆ 0.0        │\n",
      "│ mean       ┆ 572286.914333 ┆ 19.941672 ┆ 1.0        │\n",
      "│ std        ┆ 47163.728377  ┆ 9.643425  ┆ 0.0        │\n",
      "│ min        ┆ 490554.0      ┆ 9.0       ┆ 1.0        │\n",
      "│ max        ┆ 654092.0      ┆ 47.0      ┆ 1.0        │\n",
      "│ median     ┆ 572259.5      ┆ 16.0      ┆ 1.0        │\n",
      "└────────────┴───────────────┴───────────┴────────────┘\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 4/20 [25:34<1:42:45, 385.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "../data/rank_train_data_v2/train/part_5.parquet\n",
      "Original negative num: 28108705\n",
      "Validating\n",
      "(2892439, 11)\n",
      "Memory: 0.22627868875861168\n",
      "0.050644110385733285\n",
      "shape: (7, 4)\n",
      "┌────────────┬───────────────┬───────────┬────────────┐\n",
      "│ describe   ┆ session_id    ┆ rec_cnt   ┆ target_num │\n",
      "│ ---        ┆ ---           ┆ ---       ┆ ---        │\n",
      "│ str        ┆ f64           ┆ f64       ┆ f64        │\n",
      "╞════════════╪═══════════════╪═══════════╪════════════╡\n",
      "│ count      ┆ 146485.0      ┆ 146485.0  ┆ 146485.0   │\n",
      "│ null_count ┆ 0.0           ┆ 0.0       ┆ 0.0        │\n",
      "│ mean       ┆ 735873.186292 ┆ 19.745633 ┆ 1.0        │\n",
      "│ std        ┆ 47216.481088  ┆ 9.522173  ┆ 0.0        │\n",
      "│ min        ┆ 654093.0      ┆ 9.0       ┆ 1.0        │\n",
      "│ max        ┆ 817747.0      ┆ 47.0      ┆ 1.0        │\n",
      "│ median     ┆ 735805.0      ┆ 16.0      ┆ 1.0        │\n",
      "└────────────┴───────────────┴───────────┴────────────┘\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 5/20 [32:13<1:37:32, 390.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "../data/rank_train_data_v2/train/part_6.parquet\n"
     ]
    }
   ],
   "source": [
    "if debug:\n",
    "    train_parts = 1\n",
    "for part_index in tqdm(range(train_parts)):\n",
    "    print()\n",
    "    file_name = os.path.join(train_dir, f\"part_{part_index+1}.parquet\")\n",
    "    print(file_name)\n",
    "    train_cg_pl = get_all_candidates(data_type='train', part=part_index+1)\n",
    "    sampled_train_candidates = get_feature(target_df=train_cg_pl, data_type='train', \n",
    "                                           negative_sampling=0.1\n",
    "                                          )\n",
    "    sampled_train_candidates = sampled_train_candidates.collect()\n",
    "    print('Validating')\n",
    "    candidates_validation(target_pl=sampled_train_candidates)\n",
    "    # print('Sampled train data')\n",
    "    print(sampled_train_candidates.shape)\n",
    "    print(f\"Memory: {sampled_train_candidates.estimated_size(unit='gb')}\")\n",
    "    print(sampled_train_candidates['target'].mean())\n",
    "    \n",
    "    stat_info = (\n",
    "            sampled_train_candidates\n",
    "                .groupby('session_id')\n",
    "                .agg(\n",
    "                    pl.count().alias('rec_cnt')\n",
    "                    , pl.col('target').sum().alias('target_num')\n",
    "                )\n",
    "                .describe()\n",
    "        )\n",
    "    print(stat_info)\n",
    "    if not debug:\n",
    "        sampled_train_candidates.write_parquet(file_name)\n",
    "    # del sampled_train_candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ba5b6b-3d1c-49fd-a435-1725647072c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "07d623fe-32db-473e-a861-d50123d8d327",
   "metadata": {},
   "source": [
    "# Slow speed -> **7 hours to finished all of the data**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "492e83bb-c602-4189-b59c-587095798d0d",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "```\n",
    "../data/rank_train_data_v2/train/part_1.parquet\n",
    "Validating\n",
    "(28566071, 11)\n",
    "Memory: 2.23475506529212\n",
    "0.005130737090165463\n",
    "shape: (7, 4)\n",
    "┌────────────┬──────────────┬───────────┬────────────┐\n",
    "│ describe   ┆ session_id   ┆ rec_cnt   ┆ target_num │\n",
    "│ ---        ┆ ---          ┆ ---       ┆ ---        │\n",
    "│ str        ┆ f64          ┆ f64       ┆ f64        │\n",
    "╞════════════╪══════════════╪═══════════╪════════════╡\n",
    "│ count      ┆ 146565.0     ┆ 146565.0  ┆ 146565.0   │\n",
    "│ null_count ┆ 0.0          ┆ 0.0       ┆ 0.0        │\n",
    "│ mean       ┆ 81754.567209 ┆ 194.90377 ┆ 1.0        │\n",
    "│ std        ┆ 47232.319874 ┆ 96.523984 ┆ 0.0        │\n",
    "│ min        ┆ 0.0          ┆ 80.0      ┆ 1.0        │\n",
    "│ max        ┆ 163580.0     ┆ 468.0     ┆ 1.0        │\n",
    "│ median     ┆ 81763.0      ┆ 154.0     ┆ 1.0        │\n",
    "└────────────┴──────────────┴───────────┴────────────┘\n",
    "  5%|▌         | 1/20 [22:21<7:04:44, 1341.26s/it]\n",
    "\n",
    "../data/rank_train_data_v2/train/part_2.parquet\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c7b393-6ad4-4d5c-b403-51e11a848c17",
   "metadata": {},
   "source": [
    "# Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d631019-097e-4c4f-9b2b-093d80413f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! ls {ranker_train_data_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4b4de7-74d9-4bd7-b4bf-7651f632cbaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! rm {ranker_train_data_dir}/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb0702b-b60c-414e-a345-556aaaf4c35a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pl.scan_parquet(os.path.join(ranker_train_data_dir, 'test.parquet')).collect().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed88d47-4e47-4197-8a1b-6c4a7f996086",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pl.scan_parquet(os.path.join(ranker_train_data_dir, 'eval.parquet')).collect().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4364014-e8ed-440b-8eb0-6630c143c7d8",
   "metadata": {},
   "source": [
    "# Debug "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2e99e89c-e955-40d5-9eb7-57523560a244",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "../data/rank_train_data_v2/train/part_1.parquet\n",
      "Validating\n",
      "(335696, 11)\n",
      "Memory: 0.02626188099384308\n",
      "0.010298007721271627\n",
      "shape: (7, 4)\n",
      "┌────────────┬─────────────┬───────────┬────────────┐\n",
      "│ describe   ┆ session_id  ┆ rec_cnt   ┆ target_num │\n",
      "│ ---        ┆ ---         ┆ ---       ┆ ---        │\n",
      "│ str        ┆ f64         ┆ f64       ┆ f64        │\n",
      "╞════════════╪═════════════╪═══════════╪════════════╡\n",
      "│ count      ┆ 3457.0      ┆ 3457.0    ┆ 3457.0     │\n",
      "│ null_count ┆ 0.0         ┆ 0.0       ┆ 0.0        │\n",
      "│ mean       ┆ 5540.907723 ┆ 97.106161 ┆ 1.0        │\n",
      "│ std        ┆ 3186.084851 ┆ 1.80037   ┆ 0.0        │\n",
      "│ min        ┆ 5.0         ┆ 79.0      ┆ 1.0        │\n",
      "│ max        ┆ 11052.0     ┆ 100.0     ┆ 1.0        │\n",
      "│ median     ┆ 5470.0      ┆ 98.0      ┆ 1.0        │\n",
      "└────────────┴─────────────┴───────────┴────────────┘\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr > th,\n",
       ".dataframe > tbody > tr > td {\n",
       "  text-align: right;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (7, 2)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>describe</th><th>rec_num</th></tr><tr><td>str</td><td>f64</td></tr></thead><tbody><tr><td>&quot;count&quot;</td><td>10000.0</td></tr><tr><td>&quot;null_count&quot;</td><td>0.0</td></tr><tr><td>&quot;mean&quot;</td><td>97.7283</td></tr><tr><td>&quot;std&quot;</td><td>1.868804</td></tr><tr><td>&quot;min&quot;</td><td>79.0</td></tr><tr><td>&quot;max&quot;</td><td>100.0</td></tr><tr><td>&quot;median&quot;</td><td>98.0</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (7, 2)\n",
       "┌────────────┬──────────┐\n",
       "│ describe   ┆ rec_num  │\n",
       "│ ---        ┆ ---      │\n",
       "│ str        ┆ f64      │\n",
       "╞════════════╪══════════╡\n",
       "│ count      ┆ 10000.0  │\n",
       "│ null_count ┆ 0.0      │\n",
       "│ mean       ┆ 97.7283  │\n",
       "│ std        ┆ 1.868804 │\n",
       "│ min        ┆ 79.0     │\n",
       "│ max        ┆ 100.0    │\n",
       "│ median     ┆ 98.0     │\n",
       "└────────────┴──────────┘"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# part_index = 0\n",
    "# print()\n",
    "# file_name = os.path.join(train_dir, f\"part_{part_index+1}.parquet\")\n",
    "# print(file_name)\n",
    "# train_cg_pl = get_all_candidates(data_type='train', part=part_index+1)\n",
    "# sampled_train_candidates = get_feature(target_df=train_cg_pl, data_type='train', \n",
    "#                                        # negative_sampling=0.1\n",
    "#                                       )\n",
    "# sampled_train_candidates = sampled_train_candidates.collect()\n",
    "# print('Validating')\n",
    "# candidates_validation(target_pl=sampled_train_candidates)\n",
    "# # print('Sampled train data')\n",
    "# print(sampled_train_candidates.shape)\n",
    "# print(f\"Memory: {sampled_train_candidates.estimated_size(unit='gb')}\")\n",
    "# print(sampled_train_candidates['target'].mean())\n",
    "\n",
    "# stat_info = (\n",
    "#         sampled_train_candidates\n",
    "#             .groupby('session_id')\n",
    "#             .agg(\n",
    "#                 pl.count().alias('rec_cnt')\n",
    "#                 , pl.col('target').sum().alias('target_num')\n",
    "#             )\n",
    "#             .describe()\n",
    "#     )\n",
    "# print(stat_info)\n",
    "# (\n",
    "#         train_cg_pl.select(\n",
    "#                 pl.col('next_item_prediction').arr.lengths().alias('rec_num')\n",
    "#             )\n",
    "#             .collect()\n",
    "#             .describe(\n",
    "#                 # percentiles=[0.1*i for i in range(10)]\n",
    "#             )\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0d2db0b0-fa3d-4fa6-bf4d-91f38d88f768",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr > th,\n",
       ".dataframe > tbody > tr > td {\n",
       "  text-align: right;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (5, 8)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>session_id</th><th>next_item_prediction</th><th>w2v_weight</th><th>next_item</th><th>locale</th><th>prev_length</th><th>last_prev_item</th><th>rec_num</th></tr><tr><td>i64</td><td>list[str]</td><td>list[f64]</td><td>str</td><td>str</td><td>u32</td><td>str</td><td>u32</td></tr></thead><tbody><tr><td>9184</td><td>[&quot;B08447LBF5&quot;, &quot;B073VPDRP1&quot;, … &quot;B08FB3KWXL&quot;]</td><td>[0.868687, 0.515152, … 0.828283]</td><td>&quot;B017RJW8OW&quot;</td><td>&quot;DE&quot;</td><td>3</td><td>&quot;B09XF26MW6&quot;</td><td>100</td></tr><tr><td>5152</td><td>[&quot;B01LZPR94B&quot;, &quot;B0B6H7NDBS&quot;, … &quot;B0B9X73PB5&quot;]</td><td>[0.787879, 0.515152, … 0.79798]</td><td>&quot;B0B8S79JGZ&quot;</td><td>&quot;DE&quot;</td><td>3</td><td>&quot;B0BJPZHZRB&quot;</td><td>100</td></tr><tr><td>5920</td><td>[&quot;B09MHXJ4QK&quot;, &quot;B00C81E9OW&quot;, … &quot;B0B372LV18&quot;]</td><td>[0.393939, 0.585859, … 0.262626]</td><td>&quot;B098DY9JS2&quot;</td><td>&quot;DE&quot;</td><td>2</td><td>&quot;B094TZ4V98&quot;</td><td>100</td></tr><tr><td>8480</td><td>[&quot;B01E5A7KIK&quot;, &quot;B094JG3PBD&quot;, … &quot;B07Z9H23XK&quot;]</td><td>[0.181818, 0.373737, … 0.515152]</td><td>&quot;B014PI4HGK&quot;</td><td>&quot;DE&quot;</td><td>2</td><td>&quot;B0B5GZYMJH&quot;</td><td>100</td></tr><tr><td>1152</td><td>[&quot;B095R6SRG7&quot;, &quot;B07NY59JGM&quot;, … &quot;B08BXH5P73&quot;]</td><td>[0.353535, 0.464646, … 0.262626]</td><td>&quot;B07WPY21TW&quot;</td><td>&quot;DE&quot;</td><td>2</td><td>&quot;B00FYV34FI&quot;</td><td>100</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (5, 8)\n",
       "┌────────────┬────────────┬───────────────┬────────────┬────────┬───────────┬────────────┬─────────┐\n",
       "│ session_id ┆ next_item_ ┆ w2v_weight    ┆ next_item  ┆ locale ┆ prev_leng ┆ last_prev_ ┆ rec_num │\n",
       "│ ---        ┆ prediction ┆ ---           ┆ ---        ┆ ---    ┆ th        ┆ item       ┆ ---     │\n",
       "│ i64        ┆ ---        ┆ list[f64]     ┆ str        ┆ str    ┆ ---       ┆ ---        ┆ u32     │\n",
       "│            ┆ list[str]  ┆               ┆            ┆        ┆ u32       ┆ str        ┆         │\n",
       "╞════════════╪════════════╪═══════════════╪════════════╪════════╪═══════════╪════════════╪═════════╡\n",
       "│ 9184       ┆ [\"B08447LB ┆ [0.868687,    ┆ B017RJW8OW ┆ DE     ┆ 3         ┆ B09XF26MW6 ┆ 100     │\n",
       "│            ┆ F5\", \"B073 ┆ 0.515152, …   ┆            ┆        ┆           ┆            ┆         │\n",
       "│            ┆ VPDRP1\", … ┆ 0.828283]     ┆            ┆        ┆           ┆            ┆         │\n",
       "│            ┆ \"…         ┆               ┆            ┆        ┆           ┆            ┆         │\n",
       "│ 5152       ┆ [\"B01LZPR9 ┆ [0.787879,    ┆ B0B8S79JGZ ┆ DE     ┆ 3         ┆ B0BJPZHZRB ┆ 100     │\n",
       "│            ┆ 4B\", \"B0B6 ┆ 0.515152, …   ┆            ┆        ┆           ┆            ┆         │\n",
       "│            ┆ H7NDBS\", … ┆ 0.79798]      ┆            ┆        ┆           ┆            ┆         │\n",
       "│            ┆ \"…         ┆               ┆            ┆        ┆           ┆            ┆         │\n",
       "│ 5920       ┆ [\"B09MHXJ4 ┆ [0.393939,    ┆ B098DY9JS2 ┆ DE     ┆ 2         ┆ B094TZ4V98 ┆ 100     │\n",
       "│            ┆ QK\", \"B00C ┆ 0.585859, …   ┆            ┆        ┆           ┆            ┆         │\n",
       "│            ┆ 81E9OW\", … ┆ 0.262626]     ┆            ┆        ┆           ┆            ┆         │\n",
       "│            ┆ \"…         ┆               ┆            ┆        ┆           ┆            ┆         │\n",
       "│ 8480       ┆ [\"B01E5A7K ┆ [0.181818,    ┆ B014PI4HGK ┆ DE     ┆ 2         ┆ B0B5GZYMJH ┆ 100     │\n",
       "│            ┆ IK\", \"B094 ┆ 0.373737, …   ┆            ┆        ┆           ┆            ┆         │\n",
       "│            ┆ JG3PBD\", … ┆ 0.515152]     ┆            ┆        ┆           ┆            ┆         │\n",
       "│            ┆ \"…         ┆               ┆            ┆        ┆           ┆            ┆         │\n",
       "│ 1152       ┆ [\"B095R6SR ┆ [0.353535,    ┆ B07WPY21TW ┆ DE     ┆ 2         ┆ B00FYV34FI ┆ 100     │\n",
       "│            ┆ G7\", \"B07N ┆ 0.464646, …   ┆            ┆        ┆           ┆            ┆         │\n",
       "│            ┆ Y59JGM\", … ┆ 0.262626]     ┆            ┆        ┆           ┆            ┆         │\n",
       "│            ┆ \"…         ┆               ┆            ┆        ┆           ┆            ┆         │\n",
       "└────────────┴────────────┴───────────────┴────────────┴────────┴───────────┴────────────┴─────────┘"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (train_cg_pl.with_columns(\n",
    "#         pl.col('next_item_prediction').arr.lengths().alias('rec_num')\n",
    "#     )\n",
    "#     .filter(pl.col('rec_num') == 100)\n",
    "#     .head().collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "291440b1-1799-4fb0-8713-bcf2be340b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# search_id = 9184"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "fd3b95db-029f-4996-bcd7-228044ea7da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_type = 'train'\n",
    "# part = 0 +1 \n",
    "# w2v_file = os.path.join(candidate_path, \n",
    "#                        candidate_file_name.format(\n",
    "#                 task=task\n",
    "#             , data_type=data_type\n",
    "#             , model_version=w2v_model_version\n",
    "#             , model_for_eval=model_for_eval\n",
    "#             , topn=w2v_topn\n",
    "#                        ))\n",
    "# nic_file = os.path.join(candidate_path,\n",
    "#             candidate_file_name.format(\n",
    "#                 task=task\n",
    "#                 , data_type=data_type\n",
    "#                 , model_version=nic_model_version\n",
    "#                 , model_for_eval=model_for_eval\n",
    "#                 , topn=nic_topn\n",
    "#                        ))\n",
    "# nfi_file = os.path.join(candidate_path,\n",
    "#         candidate_file_name.format(\n",
    "#             task=task\n",
    "#             , data_type=data_type\n",
    "#             , model_version=nfi_model_version\n",
    "#             , model_for_eval=model_for_eval\n",
    "#             , topn=nfi_topn\n",
    "#                    ))\n",
    "# covisit_file = os.path.join(candidate_path,\n",
    "#         candidate_file_name.format(\n",
    "#             task=task\n",
    "#             , data_type=data_type\n",
    "#             , model_version=covisit_model_version\n",
    "#             , model_for_eval=model_for_eval\n",
    "#             , topn=covisit_topn\n",
    "#                    ))\n",
    "# if data_type == 'train':\n",
    "#     assert part is not None\n",
    "#     w2v_file = os.path.join(w2v_file.replace('.parquet', ''), f\"part_{part}.parquet\")\n",
    "#     nic_file = os.path.join(nic_file.replace('.parquet', ''), f\"part_{part}.parquet\")\n",
    "#     nfi_file = os.path.join(nfi_file.replace('.parquet', ''), f\"part_{part}.parquet\")\n",
    "#     covisit_file = os.path.join(covisit_file.replace('.parquet', ''), f\"part_{part}.parquet\")\n",
    "\n",
    "# if data_type == 'test':\n",
    "#     original_file_name = f\"sessions_{data_type}_{task}.parquet\"\n",
    "# else:\n",
    "#     original_file_name = f\"sessions_{data_type}.parquet\"\n",
    "# original_pl = pl.scan_parquet(os.path.join(base_dir, raw_data_session_id_dir, original_file_name), n_rows=SAMPLE_NUM).filter(pl.col('locale').is_in(target_locals)).with_columns(pl.col('prev_items').apply(str2list))\n",
    "# w2v_pl = pl.scan_parquet(w2v_file, n_rows=SAMPLE_NUM)#.with_columns(pl.col('prev_items').apply(str2list))\n",
    "# nic_pl = pl.scan_parquet(nic_file, n_rows=SAMPLE_NUM)#.with_columns(pl.col('prev_items').apply(str2list))\n",
    "# nfi_pl = pl.scan_parquet(nfi_file, n_rows=SAMPLE_NUM)#.with_columns(pl.col('prev_items').apply(str2list))\n",
    "# covisit_pl = pl.scan_parquet(covisit_file, n_rows=SAMPLE_NUM)#.with_columns(pl.col('prev_items').apply(str2list))\n",
    "\n",
    "\n",
    "# #     def get_vector(prev_items):\n",
    "# #         prev_items_vec = np.array([w2vec.wv[ele] for ele in prev_items]).mean(axis=0)\n",
    "# #         return prev_items_vec\n",
    "\n",
    "# #     original_pl = (\n",
    "# #             original_pl.with_columns(\n",
    "# #                 pl.col('prev_items').apply(lambda x: get_vector(x)).alias('all_previous_items_vec')\n",
    "# #             )\n",
    "# #         )\n",
    "\n",
    "# # get w2v weight\n",
    "# w2v_pl = w2v_pl.with_columns(pl.col('next_item_prediction').arr.lengths().alias('rec_num')).with_columns(\n",
    "#     pl.col('rec_num').apply(lambda x: list(range(x, 0, -1))).alias('w2v_weight')\n",
    "\n",
    "# )\n",
    "# # print(w2v_pl.head().collect())\n",
    "# get_w2v_weight = pl.element().rank()*0\n",
    "# nic_pl = nic_pl.with_columns(\n",
    "#     pl.col('next_item_prediction').arr.eval(get_w2v_weight, \n",
    "#                                             # parallel=True\n",
    "#                                            ).alias('w2v_weight').cast(pl.List(pl.Int64))\n",
    "# )\n",
    "# nfi_pl = nfi_pl.with_columns(\n",
    "#     pl.col('next_item_prediction').arr.eval(get_w2v_weight, \n",
    "#                                            ).alias('w2v_weight').cast(pl.List(pl.Int64))\n",
    "# )\n",
    "# covisit_pl = covisit_pl.with_columns(\n",
    "#     pl.col('next_item_prediction').arr.eval(get_w2v_weight, \n",
    "#                                            ).alias('w2v_weight').cast(pl.List(pl.Int64))\n",
    "# )\n",
    "# cols = ['session_id', 'next_item_prediction', \n",
    "#         'w2v_weight'\n",
    "#        ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "42c916f7-0e25-4e88-83d9-613540059dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nic_rec = nic_pl.filter(pl.col('session_id')==search_id).select('next_item_prediction').collect().to_series().to_list()[0]\n",
    "# w2v_rec = w2v_pl.filter(pl.col('session_id')==search_id).select('next_item_prediction').collect().to_series().to_list()[0]"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "kdd_2023",
   "name": "common-cu110.m104",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cu110:m104"
  },
  "kernelspec": {
   "display_name": "py3.8(kdd_2023)",
   "language": "python",
   "name": "kdd_2023"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
