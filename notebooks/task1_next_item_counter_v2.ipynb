{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Package "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "import logging\n",
    "sys.path.append('../')\n",
    "import os\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "import pickle\n",
    "import gc\n",
    "import re\n",
    "import polars as pl\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "from tqdm.auto import tqdm\n",
    "import polars as pl\n",
    "from utils import *\n",
    "from src.eval import get_recall_at_k, pd_get_recall_at_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_prod = pd.read_csv('data/products_train.csv')\n",
    "# df_prod"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Config "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug = False\n",
    "model_for_eval = False\n",
    "\n",
    "model_version = 'next_item_counter_v2'\n",
    "\n",
    "\n",
    "if debug:\n",
    "    n_rows = 1000\n",
    "else:\n",
    "    n_rows = None\n",
    "# debug_session_num = 100\n",
    "train_data_dir = '.'\n",
    "test_data_dir = '.'\n",
    "task = 'task1'\n",
    "\n",
    "model_dir = f'../model_training/{model_version}/'\n",
    "\n",
    "# target locales: locales needed for task1\n",
    "target_locals = [\"DE\", 'JP', 'UK']\n",
    "\n",
    "\n",
    "submit_file = f'submission_{task}_next_item_counter.parquet'\n",
    "\n",
    "if model_for_eval:\n",
    "    model_file = os.path.join(model_dir, 'nic_model_for_eval.parquet')\n",
    "else:\n",
    "    model_file = os.path.join(model_dir, 'nic_model.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘../model_training/next_item_counter_v2/’: File exists\n"
     ]
    }
   ],
   "source": [
    "! mkdir {model_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../model_training/next_item_counter_v2/nic_model.parquet'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# products = read_product_data(train_data_dir=train_data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr > th,\n",
       ".dataframe > tbody > tr > td {\n",
       "  text-align: right;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (3, 3)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>prev_items</th><th>next_item</th><th>locale</th></tr><tr><td>list[str]</td><td>str</td><td>str</td></tr></thead><tbody><tr><td>[&quot;B09W9FND7K&quot;, &quot;B09JSPLN1M&quot;]</td><td>&quot;B09M7GY217&quot;</td><td>&quot;DE&quot;</td></tr><tr><td>[&quot;B076THCGSG&quot;, &quot;B007MO8IME&quot;, … &quot;B001B4TKA0&quot;]</td><td>&quot;B001B4THSA&quot;</td><td>&quot;DE&quot;</td></tr><tr><td>[&quot;B0B1LGXWDS&quot;, &quot;B00AZYORS2&quot;, … &quot;B00AZYORS2&quot;]</td><td>&quot;B0767DTG2Q&quot;</td><td>&quot;DE&quot;</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (3, 3)\n",
       "┌───────────────────────────────────┬────────────┬────────┐\n",
       "│ prev_items                        ┆ next_item  ┆ locale │\n",
       "│ ---                               ┆ ---        ┆ ---    │\n",
       "│ list[str]                         ┆ str        ┆ str    │\n",
       "╞═══════════════════════════════════╪════════════╪════════╡\n",
       "│ [\"B09W9FND7K\", \"B09JSPLN1M\"]      ┆ B09M7GY217 ┆ DE     │\n",
       "│ [\"B076THCGSG\", \"B007MO8IME\", … \"… ┆ B001B4THSA ┆ DE     │\n",
       "│ [\"B0B1LGXWDS\", \"B00AZYORS2\", … \"… ┆ B0767DTG2Q ┆ DE     │\n",
       "└───────────────────────────────────┴────────────┴────────┘"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_sess = pl.scan_csv('sessions_train.csv', n_rows=n_rows).filter(pl.col('locale').is_in(target_locals)).with_columns(pl.col('prev_items').apply(str2list))\n",
    "df_sess.head(3).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<h4>NAIVE QUERY PLAN</h4><p>run <b>LazyFrame.show_graph()</b> to see the optimized version</p><?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n",
       " -->\n",
       "<!-- Title: polars_query Pages: 1 -->\n",
       "<svg width=\"285pt\" height=\"133pt\"\n",
       " viewBox=\"0.00 0.00 285.00 133.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 129)\">\n",
       "<title>polars_query</title>\n",
       "<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-129 281,-129 281,4 -4,4\"/>\n",
       "<!-- WITH COLUMNS [&quot;prev_items&quot;] [(0, 0)] -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>WITH COLUMNS [&quot;prev_items&quot;] [(0, 0)]</title>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"265.5,-125 11.5,-125 11.5,-89 265.5,-89 265.5,-125\"/>\n",
       "<text text-anchor=\"middle\" x=\"138.5\" y=\"-103.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">WITH COLUMNS [&quot;prev_items&quot;]</text>\n",
       "</g>\n",
       "<!-- CSV SCAN sessions_test_task1.csv;\n",
       "π */2;\n",
       "σ &#45; [(0, 1)] -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>CSV SCAN sessions_test_task1.csv;\n",
       "π */2;\n",
       "σ &#45; [(0, 1)]</title>\n",
       "<polygon fill=\"none\" stroke=\"#000000\" points=\"277,-53 0,-53 0,0 277,0 277,-53\"/>\n",
       "<text text-anchor=\"middle\" x=\"138.5\" y=\"-37.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">CSV SCAN sessions_test_task1.csv;</text>\n",
       "<text text-anchor=\"middle\" x=\"138.5\" y=\"-22.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">π */2;</text>\n",
       "<text text-anchor=\"middle\" x=\"138.5\" y=\"-7.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">σ &#45;</text>\n",
       "</g>\n",
       "<!-- WITH COLUMNS [&quot;prev_items&quot;] [(0, 0)]&#45;&#45;CSV SCAN sessions_test_task1.csv;\n",
       "π */2;\n",
       "σ &#45; [(0, 1)] -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>WITH COLUMNS [&quot;prev_items&quot;] [(0, 0)]&#45;&#45;CSV SCAN sessions_test_task1.csv;\n",
       "π */2;\n",
       "σ &#45; [(0, 1)]</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M138.5,-88.7333C138.5,-78.2495 138.5,-64.8954 138.5,-53.2477\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<polars.LazyFrame object at 0x7FECC20AF700>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = pl.scan_csv('sessions_test_task1.csv', n_rows=n_rows).with_columns(pl.col('prev_items').apply(str2list))\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_sess = df_sess[df_sess['locale'].isin(target_locals)]\n",
    "\n",
    "# if debug:\n",
    "#     df_sess = df_sess.sample(debug_session_num)\n",
    "#     df_test = df_test.sample(debug_session_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next Item Statistics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pl = df_sess.with_columns(\n",
    "    pl.col('prev_items').arr.concat(pl.col('next_item')) \n",
    ")\n",
    "test_pl = df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_pl.head().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if not model_for_eval:\n",
    "\n",
    "\n",
    "cols_to_keep = ['prev_items']\n",
    "next_items_pl = (\n",
    "    pl.concat([train_pl.select(cols_to_keep), test_pl.select(cols_to_keep)], how='vertical')\n",
    "        .with_columns(\n",
    "            pl.col('prev_items').arr.shift(-1).alias('next_item_lst')\n",
    "            , pl.col('prev_items').arr.lengths().alias('length')\n",
    "        )\n",
    "        .select(\n",
    "            pl.col('prev_items').arr.head(pl.col('length')-1).alias('prev')\n",
    "            , pl.col('next_item_lst').arr.head(pl.col('length')-1).alias('next')\n",
    "        )\n",
    "        .explode(['prev','next' ])\n",
    "        .groupby(['prev','next' ])\n",
    "        .agg(\n",
    "            pl.count().alias('cnt')\n",
    "        )\n",
    "        .sort(['prev', 'cnt'], descending=True)\n",
    "        .with_columns(\n",
    "            pl.col('cnt').max().over('prev').alias('max_count')\n",
    "            , pl.col('cnt').min().over('prev').alias('min_count')\n",
    "        )\n",
    "        .with_columns(\n",
    "            pl.when(pl.col('max_count')==pl.col('min_count')).then(1).otherwise((pl.col('cnt')-pl.col('min_count'))/(pl.col('max_count')-pl.col('min_count'))).alias('normalized_cnt')\n",
    "        )\n",
    "        .groupby('prev')\n",
    "        .agg(\n",
    "            pl.col('next').alias('next_item_prediction')\n",
    "            , pl.col('cnt').alias('next_item_cnt')\n",
    "            , (pl.col('normalized_cnt')+pl.lit(0.01)).alias('next_item_weight')\n",
    "        )\n",
    "        .select(\n",
    "            pl.col('prev').alias('item')\n",
    "            , 'next_item_prediction'\n",
    "            , pl.col('next_item_weight')\n",
    "        )\n",
    "        \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next_items_pl.collect()#.filter(pl.col('item')=='B09LXX1PQ9')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next_item_dict = defaultdict(list)\n",
    "\n",
    "# for _, row in tqdm(df_sess.iterrows(), total=len(df_sess)):\n",
    "#     prev_items = str2list(row['prev_items'])\n",
    "#     if not model_for_eval:\n",
    "#         next_item = row['next_item']\n",
    "#     prev_items_length = len(prev_items)\n",
    "#     if prev_items_length <= 1:\n",
    "#         if not model_for_eval:\n",
    "#             next_item_dict[prev_items[0]].append(next_item)\n",
    "#     else:\n",
    "#         for i, item in enumerate(prev_items[:-1]):\n",
    "#             next_item_dict[item].append(prev_items[i+1])\n",
    "#         if not model_for_eval:\n",
    "#             next_item_dict[prev_items[-1]].append(next_item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next_item_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for _, row in tqdm(df_test.iterrows(), total=len(df_test)):\n",
    "#     prev_items = str2list(row['prev_items'])\n",
    "#     prev_items_length = len(prev_items)\n",
    "#     if prev_items_length <= 1:\n",
    "#         continue\n",
    "#     else:\n",
    "#         for i, item in enumerate(prev_items[:-1]):\n",
    "#             next_item_dict[item].append(prev_items[i+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # next_item_map = {}\n",
    "# topn = 100\n",
    "# item_lst = []\n",
    "# common_items_lst = []\n",
    "# weights_lst = []\n",
    "# for item in tqdm(next_item_dict):\n",
    "#     counter = Counter(next_item_dict[item])\n",
    "#     most_common_cnt = counter.most_common(1)[0][1]\n",
    "#     most_common_lst = list(zip(*counter.most_common(topn)))\n",
    "#     most_common_lst[1] = list(np.array(most_common_lst[1])/most_common_cnt)\n",
    "#     item_lst.append(item)\n",
    "#     common_items_lst.append(list(most_common_lst[0]))\n",
    "#     weights_lst.append(most_common_lst[1])\n",
    "#     # next_item_map[item] = most_common_lst\n",
    "#     # next_item_map[item] = [i[0] for i in counter.most_common(100)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next_item_df = pd.DataFrame(\n",
    "#     {'item': item_lst\n",
    "#     , 'next_item_prediction': common_items_lst\n",
    "#      , 'next_item_weight': weights_lst\n",
    "#     }\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next_item_df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next_items_pl.collect().filter(pl.col('item')=='B07QGW8LFT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nex_item_pl = pl.from_pandas(next_item_df).lazy().select(\n",
    "#     'item'\n",
    "#     , pl.col('next_item_prediction').alias('next_item_rec')\n",
    "#     , 'next_item_weight'\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('../model_training/next_item_counter/model.pkl', 'rb') as f:\n",
    "#     model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model['next_item_map']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nex_item_pl = pl.DataFrame(\n",
    "#     {\n",
    "#         'item': model['next_item_map'].keys()\n",
    "#         , 'next_item_rec': model['next_item_map'].values()\n",
    "#     }\n",
    "# ).lazy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1270206, 3)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_items_pl.collect().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Save model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_items_pl.collect().write_parquet(model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "del next_items_pl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_items_pl = pl.scan_parquet(model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'item': Utf8,\n",
       " 'next_item_prediction': List(Utf8),\n",
       " 'next_item_weight': List(Float64)}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_items_pl.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Top200 for fallback logics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next_item_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next_item_df[cols].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'LazyFrame' object is not subscriptable (aside from slicing). Use 'select()' or 'filter()' instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m popular_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([\u001b[43mdf_sess\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mprev_items\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlocale\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m, df_test[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprev_items\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlocale\u001b[39m\u001b[38;5;124m'\u001b[39m]]], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/kdd-2023-KklMGVX0-py3.8/lib/python3.8/site-packages/polars/lazyframe/frame.py:656\u001b[0m, in \u001b[0;36mLazyFrame.__getitem__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m    654\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, item: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mrange\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Self:\n\u001b[1;32m    655\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(item, \u001b[38;5;28mslice\u001b[39m):\n\u001b[0;32m--> 656\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    657\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLazyFrame\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object is not subscriptable (aside from slicing). Use\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    658\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mselect()\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfilter()\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    659\u001b[0m         )\n\u001b[1;32m    660\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_from_pyldf(LazyPolarsSlice(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mapply(item)\u001b[38;5;241m.\u001b[39m_ldf)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'LazyFrame' object is not subscriptable (aside from slicing). Use 'select()' or 'filter()' instead."
     ]
    }
   ],
   "source": [
    "popular_df = pd.concat([df_sess[['prev_items', 'locale']], df_test[['prev_items', 'locale']]], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "popular_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "popular_pl = pl.from_pandas(popular_df).lazy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topn = 200\n",
    "locale_popular_pl = (\n",
    "    popular_pl\n",
    "        .select(\n",
    "            pl.col('prev_items').apply(str2list)#.explode().alias('item')\n",
    "            , pl.col('locale')\n",
    "        )\n",
    "        .explode('prev_items')#.alias('item')\n",
    "        .groupby(['locale', 'prev_items'])\n",
    "        .agg(\n",
    "            pl.count()\n",
    "        )\n",
    "        .with_columns(\n",
    "            pl.col('count').rank(method='ordinal', descending=True).over('locale').alias('rank')\n",
    "        )\n",
    "        .filter(pl.col('rank')<=topn)\n",
    "        .with_columns(\n",
    "            pl.col('count').max().over('locale').alias('max_count')\n",
    "            , pl.col('count').min().over('locale').alias('min_count')\n",
    "        )\n",
    "        .with_columns(\n",
    "            ((pl.col('count')-pl.col('min_count'))/(pl.col('max_count')-pl.col('min_count'))).alias('weight')\n",
    "        )\n",
    "        .sort('locale', 'rank')\n",
    "        .select(\n",
    "            'locale'\n",
    "            , 'prev_items'\n",
    "            , 'weight'\n",
    "        )\n",
    "        .groupby('locale')\n",
    "        .agg(\n",
    "            pl.col('weight').alias('locale_popular_weight')\n",
    "            , pl.col('prev_items').alias('locale_popular_rec')\n",
    "        )\n",
    "        # .count()#.head(3).collect())\n",
    "        # .collect()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# locale_popular_pl.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# locale_popular_pl.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# popular_df.apply(lambda x: str2list(x['prev_items']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_sess.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # next_item_df['next_item_prediction'] = next_item_df['next_item_prediction'].astype(str)\n",
    "# # next_item_df['next_item_weights'] = next_item_df['next_item_weights'].astype(str)\n",
    "# cols = [\n",
    "#     # 'item',\n",
    "#         'next_item_prediction'\n",
    "#         , 'next_item_weights'\n",
    "#        ]\n",
    "# next_item_pl = pl.from_pandas(next_item_df[cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next_item_pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # k = []\n",
    "# # v = []\n",
    "\n",
    "# # for item in next_item_dict:\n",
    "# #     k.append(item)\n",
    "# #     v.append(next_item_dict[item])\n",
    "    \n",
    "# # df_next = pd.DataFrame({'item': k, 'next_item': v})\n",
    "# df_next = next_item_df.explode('next_item_prediction').reset_index(drop=True)\n",
    "# df_next = df_next.merge(products, how='left', left_on='item', right_on='id')\n",
    "# df_next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_next['next_item'].value_counts().index.tolist()[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = {\n",
    "#     'top200': top200\n",
    "#     , 'next_item_map': next_item_map\n",
    "# }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Save model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(model_file, 'wb') as f:\n",
    "#     pickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Get final result "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Load Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # with open(model_file, 'rb') as f:\n",
    "#     model = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next_item_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def get_rec(target_df, model):\n",
    "#     next_item_map = model['next_item_map']\n",
    "#     top200  = model['top200']\n",
    "#     target_df['last_item'] = target_df['prev_items'].apply(lambda x: str2list(x)[-1])\n",
    "#     target_df['next_item_prediction'] = target_df['last_item'].map(next_item_map)\n",
    "#     preds = []\n",
    "\n",
    "#     for _, row in tqdm(target_df.iterrows(), total=len(target_df)):\n",
    "#         pred_orig = row['next_item_prediction']\n",
    "#         pred = pred_orig\n",
    "#         prev_items = str2list(row['prev_items'])\n",
    "#         if type(pred) == float:\n",
    "#             pred = top200[:100]\n",
    "#         else:\n",
    "#             if len(pred_orig) < 100:\n",
    "#                 for i in top200:\n",
    "#                     if i not in pred_orig and i not in prev_items:\n",
    "#                         pred.append(i)\n",
    "#                     if len(pred) >= 100:\n",
    "#                         break\n",
    "#             else:\n",
    "#                 pred = pred[:100]\n",
    "#         preds.append(pred)\n",
    "#     target_df['next_item_prediction'] = preds\n",
    "#     print(target_df['next_item_prediction'].apply(len).describe())\n",
    "#     return target_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model['next_item_map']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Candidate for train data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_pl = pl.scan_csv('sessions_train.csv')\n",
    "train_pl = pl.scan_parquet('../data/eval_data/next_item_counter_train_eval_300k.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_locals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pl.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pl = (\n",
    "    train_pl\n",
    "        .filter(pl.col('locale').is_in(target_locals))\n",
    "        .with_columns(\n",
    "            pl.col('prev_items').apply(str2list).arr.get(-1).alias('last_item')\n",
    "        )\n",
    "        .join(nex_item_pl, how='left', left_on='last_item', right_on='item')\n",
    "        .with_columns(\n",
    "            pl.when(pl.col('next_item_rec').is_null()).then([]).otherwise(pl.col('next_item_rec').arr.head(100)).alias('next_item_prediction')\n",
    "        )\n",
    "        .with_columns(\n",
    "            pl.col('next_item_prediction').arr.lengths().alias('rec_num')\n",
    "        )\n",
    "        .select(\n",
    "            'prev_items'\n",
    "            , 'next_item'\n",
    "            , 'locale'\n",
    "            , 'next_item_prediction'\n",
    "            , 'rec_num'\n",
    "        )\n",
    ")#.head(2).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pl.collect().write_parquet('../data/candidates/task1_train_nic_without_pupular_top100_300k.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final resul "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pl_rec(target_pl, locale_popular_pl, nex_item_pl):\n",
    "    rec_num = 100\n",
    "    target_pl = (\n",
    "        target_pl\n",
    "            .with_columns(\n",
    "                pl.col('prev_items').apply(str2list).arr.get(-1).alias('last_item')\n",
    "            )\n",
    "            .join(nex_item_pl, how='left', left_on='last_item', right_on='item')\n",
    "            .join(locale_popular_pl, how='left', on='locale')\n",
    "            .with_columns(\n",
    "                pl.when(pl.col('next_item_rec').is_null()).then([]).otherwise(pl.col('next_item_rec')).alias('next_item_rec')\n",
    "            )\n",
    "            .with_columns(\n",
    "                pl.concat_list([pl.col('next_item_rec'), pl.col('locale_popular_rec')])\n",
    "                    .alias('next_item_prediction')\n",
    "                    .arr.head(rec_num)\n",
    "\n",
    "            )\n",
    "            .with_columns(\n",
    "                pl.col('next_item_prediction').arr.lengths().alias('rec_num')\n",
    "            )\n",
    "    )#.head(3).collect()\n",
    "    return target_pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_pl = pl.scan_parquet(f'../data/eval_data/w2v_train_eval_result_300k.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_pl.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nex_item_pl.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# locale_popular_pl.head(3).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_pl = pl_rec(target_pl=eval_pl, locale_popular_pl=locale_popular_pl, nex_item_pl=nex_item_pl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval_pl.head(3).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_pl.select(\n",
    "    pl.col('next_item_prediction').arr.head(20).arr.contains(pl.col('next_item')).mean().alias('recall@20')\n",
    "    , pl.col('next_item_prediction').arr.head(100).arr.contains(pl.col('next_item')).mean().alias('recall@100')\n",
    ").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# eval_cols = ['len', 'recall@20', 'recall@100']\n",
    "# train_eval_df[eval_cols] = train_eval_df.apply(pd_get_recall_at_k, axis=1, result_type='expand')\n",
    "# print(train_eval_df[eval_cols].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_eval_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_pl.collect().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_pl.collect().write_parquet(f'../data/eval_data/{model_version}_train_eval_300k.parquet', \n",
    "                      # engine='pyarrow'\n",
    "                     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submit result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pl = pl.scan_csv('sessions_test_task1.csv')\n",
    "test_pl = pl_rec(target_pl=test_pl, locale_popular_pl=locale_popular_pl, nex_item_pl=nex_item_pl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls -al | grep {submit_file}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pl.collect().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pl.head(3).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_pl.collect().select('locale', 'next_item_prediction').write_parquet(submit_file,\n",
    "#                                                                          # engine='pyarrow'\n",
    "#                                                                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # You can submit with aicrowd-cli, or upload manually on the challenge page.\n",
    "# !aicrowd submission create -c task-1-next-product-recommendation -f {submit_file}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rank  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_df = pl.scan_parquet('submission_task1.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert w2v_df.collect().shape[0] == test_pl.collect().shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_df.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_pl.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_df = pl.concat([test_pl.select('prev_items', 'locale', 'next_item_rec').collect(), w2v_df.select('next_item_prediction').collect()]\n",
    "                    , how='horizontal' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec_num = 100\n",
    "target_pl = (\n",
    "    target_df\n",
    "        .lazy()\n",
    "        .select(\n",
    "            'prev_items'\n",
    "            , 'locale'\n",
    "            , pl.concat_list([pl.col('next_item_rec'), pl.col('next_item_prediction')])\n",
    "                .alias('next_item_prediction')\n",
    "                .arr.head(rec_num)\n",
    "\n",
    "        )\n",
    "        .with_columns(\n",
    "            pl.col('next_item_prediction').arr.lengths().alias('rec_num')\n",
    "        )\n",
    ")#.head(3).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_pl.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_pl.head(6).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! mkdir ../data/sub_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_pl.collect().select('locale', 'next_item_prediction').write_parquet('../data/sub_files/rank_v1.parquet',\n",
    "                                                                         # engine='pyarrow'\n",
    "                                                                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # You can submit with aicrowd-cli, or upload manually on the challenge page.\n",
    "# !aicrowd submission create -c task-1-next-product-recommendation -f '../data/sub_files/rank_v1.parquet'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rank2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rec_num = 100\n",
    "target_pl = (\n",
    "    target_df\n",
    "        .lazy()\n",
    "        .select(\n",
    "            'prev_items'\n",
    "            , 'locale'\n",
    "            , pl.concat_list([pl.col('next_item_rec').arr.head(20), pl.col('next_item_prediction')])\n",
    "                .alias('next_item_prediction')\n",
    "                .arr.head(rec_num)\n",
    "\n",
    "        )\n",
    "        .with_columns(\n",
    "            pl.col('next_item_prediction').arr.lengths().alias('rec_num')\n",
    "        )\n",
    ")#.head(3).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_pl.collect().select('locale', 'next_item_prediction').write_parquet('../data/sub_files/rank_v2.parquet',\n",
    "                                                                         # engine='pyarrow'\n",
    "                                                                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # You can submit with aicrowd-cli, or upload manually on the challenge page.\n",
    "!aicrowd submission create -c task-1-next-product-recommendation -f '../data/sub_files/rank_v2.parquet'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Debug "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pl = pl.scan_parquet(submit_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pl.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pl.head(5).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pl.select(\n",
    "    pl.col('next_item_prediction').arr.lengths().min()\n",
    "    , pl.col('next_item_prediction').arr.lengths().max()\n",
    ").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "kdd_2023",
   "name": "common-cu110.m104",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cu110:m104"
  },
  "kernelspec": {
   "display_name": "py3.8(kdd_2023)",
   "language": "python",
   "name": "kdd_2023"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
