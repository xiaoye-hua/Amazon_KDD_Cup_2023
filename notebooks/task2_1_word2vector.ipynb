{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic ideas "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Word2vector\n",
    "2. Next steps\n",
    "    1. [x] train2 data should be included\n",
    "    2. [x] a unified model for both task1 and task2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LmjiT10Qk5m8",
    "tags": []
   },
   "source": [
    "# Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "7DjmcQMAgPAJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "import logging\n",
    "base_dir = '../'\n",
    "\n",
    "sys.path.append(base_dir)\n",
    "import os\n",
    "from gensim.similarities.annoy import AnnoyIndexer\n",
    "\n",
    "from utils import *\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gensim.models import Word2Vec\n",
    "from tqdm import tqdm\n",
    "import polars as pl\n",
    "from annoy import AnnoyIndex\n",
    "import polars as pl\n",
    "import polars as pl\n",
    "from utils import *\n",
    "from src.eval import model_eval\n",
    "from src.config import (\n",
    "    raw_data_session_id_dir, candidate_dir, model_for_eval, candidate_file_name, submit_file_name\n",
    "    # , w2v_model_file_name, w2v_index_file_name\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JxcGbj4xAqqe"
   },
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "qYPjjtQ_AqRT"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/candidates/task1_train_w2v_task1_v10_True_top100.parquet\n",
      "../data/candidates/task1_eval_w2v_task1_v10_True_top100.parquet\n",
      "../data/candidates/task1_test_w2v_task1_v10_True_top100.parquet\n",
      "../data/candidates/task1_test4task3_w2v_task1_v10_True_top100.parquet\n"
     ]
    }
   ],
   "source": [
    "debug = False\n",
    "\n",
    "topn = 100\n",
    "\n",
    "task = 'task1'\n",
    "\n",
    "version = 'v10'\n",
    "target_locals = [\"DE\", 'JP', 'UK']\n",
    "\n",
    "model_version = f'w2v_{task}_{version}'\n",
    "unified_model = True\n",
    "\n",
    "# target_locals = ['ES', 'FR', 'IT']\n",
    "\n",
    "\n",
    "\n",
    "if unified_model:\n",
    "    train_target_locals = ['ES', 'FR', 'IT', \"DE\", 'JP', 'UK']\n",
    "else:\n",
    "    train_target_locals = target_locals\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "if debug:\n",
    "    n_rows = 100\n",
    "else:\n",
    "    n_rows = None\n",
    "\n",
    "    \n",
    "\n",
    "# Common setting\n",
    "# target locales: locales needed for task1\n",
    "submit_file = os.path.join('../data/sub_files/', \n",
    "                           submit_file_name.format(\n",
    "                                task=task\n",
    "                               , model_version=model_version\n",
    "                               , model_for_eval=model_for_eval\n",
    "                               , topn=topn\n",
    "                           )\n",
    "                          )\n",
    "\n",
    "train_cg_file = os.path.join(base_dir,\n",
    "                             candidate_dir, \n",
    "            candidate_file_name.format(\n",
    "                task=task\n",
    "                , data_type='train'\n",
    "                , model_version=model_version\n",
    "                , model_for_eval=model_for_eval\n",
    "                , topn=topn\n",
    "            )\n",
    "                            )\n",
    "eval_cg_file = os.path.join(base_dir,\n",
    "                            candidate_dir, \n",
    "            candidate_file_name.format(\n",
    "                task=task\n",
    "                , data_type='eval'\n",
    "                , model_version=model_version\n",
    "                , model_for_eval=model_for_eval\n",
    "                , topn=topn\n",
    "            )\n",
    "                            )\n",
    "test_cg_file = os.path.join(base_dir,\n",
    "                            candidate_dir, \n",
    "            candidate_file_name.format(\n",
    "                task=task\n",
    "                , data_type='test'\n",
    "                , model_version=model_version\n",
    "                , model_for_eval=model_for_eval\n",
    "                , topn=topn\n",
    "            )\n",
    "                            )\n",
    "test4task3_cg_file= os.path.join(base_dir,\n",
    "                            candidate_dir, \n",
    "                                    candidate_file_name.format(\n",
    "    task=task\n",
    "    , data_type='test4task3'\n",
    "    , model_version=model_version\n",
    "    , model_for_eval=model_for_eval\n",
    "    , topn=topn\n",
    "))\n",
    "print(train_cg_file)\n",
    "print(eval_cg_file)\n",
    "print(test_cg_file)\n",
    "print(test4task3_cg_file)\n",
    "\n",
    "\n",
    "model_version = f'w2v_task2_v2'\n",
    "model_dir = f'../model_training/{model_version}/'\n",
    "\n",
    "# specific setting\n",
    "num_tree = 100\n",
    "# if model_for_eval:\n",
    "w2v_model_file = os.path.join(model_dir, f\"{model_for_eval}.model\")\n",
    "annoy_index_file = os.path.join(model_dir, f\"{str(num_tree)}_{model_for_eval}.index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# submit_file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘../model_training/w2v_task2_v2/’: File exists\n"
     ]
    }
   ],
   "source": [
    "! mkdir {model_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "6oiTtQ56gYlY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../model_training/w2v_task2_v2/True.model; \n",
      " ../model_training/w2v_task2_v2/100_True.index; \n",
      " ../data/sub_files/submit_task1_w2v_task1_v10_True_top100.parquet  \n",
      " ../data/candidates/task1_train_w2v_task1_v10_True_top100.parquet \n",
      " ../data/candidates/task1_eval_w2v_task1_v10_True_top100.parquet \n",
      " ../data/candidates/task1_test_w2v_task1_v10_True_top100.parquet\n"
     ]
    }
   ],
   "source": [
    "print(f\"{w2v_model_file}; \\n {annoy_index_file}; \\n {submit_file}  \\n {train_cg_file} \\n {eval_cg_file} \\n {test_cg_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def w2v_rec(target_df, w2vec, annoy_index, topn=100):\n",
    "    final_cols = ['session_id', 'next_item_prediction']\n",
    "    def get_next_items(x):\n",
    "        prev_items = x['prev_items']\n",
    "        local_rec = x['next_item_prediction']\n",
    "        final = [ele for ele in local_rec if ele not in prev_items]\n",
    "        return final\n",
    "    target_df = (\n",
    "        target_df.with_columns(\n",
    "            pl.col('prev_items').apply(lambda x: \n",
    "                                       # x.to_list()\n",
    "                                       list(map(list, zip(*w2vec.wv.most_similar(positive=x.to_list(),\n",
    "                                                                     topn=topn+10,\n",
    "                                                                     indexer=annoy_index))))[0]\n",
    "                                      ).alias('next_item_prediction'))\n",
    "        .with_columns(\n",
    "                    pl.struct([\"prev_items\", \"next_item_prediction\"]).apply(\n",
    "                        lambda x: get_next_items(x)).arr.head(topn).alias('next_item_prediction'))\n",
    "        .select(\n",
    "          final_cols\n",
    "        )\n",
    "        .with_columns(\n",
    "            pl.col('next_item_prediction').arr.lengths().alias('rec_num')\n",
    "        )\n",
    "    )\n",
    "    return target_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../data/candidates/task1_test_w2v_task1_v10_True_top100.parquet'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_cg_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task2_eval_co_visit_task2_v2_True_top300.parquet\n",
      "task2_eval_nfi_task2_v2_True_top100.parquet\n",
      "task2_eval_nic_task2_v2_True_top100.parquet\n",
      "task2_eval_w2v_task2_v2_True_top100.parquet\n",
      "task2_test4task3_co_visit_task2_v2_True_top300.parquet\n",
      "task2_test4task3_nfi_task2_v2_True_top100.parquet\n",
      "task2_test4task3_nic_task2_v2_True_top100.parquet\n",
      "task2_test4task3_w2v_task2_v2_True_top100.parquet\n",
      "task2_test_co_visit_task2_v2_True_top300.parquet\n",
      "task2_test_nfi_task2_v2_True_top100.parquet\n",
      "task2_test_nic_task2_v2_True_top100.parquet\n",
      "task2_test_w2v_task2_v2_True_top100.parquet\n",
      "task2_train_co_visit_task2_v2_True_top300\n",
      "task2_train_co_visit_task2_v2_True_top300.parquet\n",
      "task2_train_nfi_task2_v2_True_top100\n",
      "task2_train_nfi_task2_v2_True_top100.parquet\n",
      "task2_train_nic_task2_v2_True_top100\n",
      "task2_train_nic_task2_v2_True_top100.parquet\n",
      "task2_train_w2v_task2_v2_True_top100\n",
      "task2_train_w2v_task2_v2_True_top100.parquet\n"
     ]
    }
   ],
   "source": [
    "! ls ../data/candidates/ | grep task2_v2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f0T1vtOEB5v-",
    "tags": []
   },
   "source": [
    "# Word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Train data process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Train model & annnoy index "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = (\n",
    "    pl.scan_parquet(os.path.join(base_dir,\n",
    "                                          raw_data_session_id_dir,\n",
    "                                          'sessions_train1.parquet'),\n",
    "                             n_rows=n_rows)\n",
    "        .filter(pl.col('locale').is_in(train_target_locals))\n",
    "        .with_columns(pl.col('prev_items').apply(str2list))\n",
    "        .with_columns(pl.col('prev_items').arr.concat(pl.col('next_item')))\n",
    ")\n",
    "if not model_for_eval:\n",
    "    eval_data = (\n",
    "        pl.scan_parquet(os.path.join(base_dir, \n",
    "                                     raw_data_session_id_dir, \n",
    "                                     'sessions_eval.parquet'),\n",
    "                        n_rows=n_rows)\n",
    "            .filter(pl.col('locale').is_in(train_target_locals))\n",
    "        .with_columns(pl.col('prev_items').apply(str2list))\n",
    "        .with_columns(\n",
    "                    pl.col('prev_items').arr.concat(pl.col('next_item')) )\n",
    "    )\n",
    "else:\n",
    "    eval_data = (\n",
    "                pl.scan_parquet(os.path.join(base_dir, \n",
    "                                     raw_data_session_id_dir, \n",
    "                                     'sessions_eval.parquet'),\n",
    "                        n_rows=n_rows)\n",
    "            .filter(pl.col('locale').is_in(train_target_locals))\n",
    "        .with_columns(pl.col('prev_items').apply(str2list))\n",
    "    )\n",
    "\n",
    "train2_data = (\n",
    "    pl.scan_parquet(os.path.join(base_dir,\n",
    "                                 raw_data_session_id_dir, \n",
    "                                 'sessions_train2.parquet'),\n",
    "                    n_rows=n_rows)\n",
    "        .filter(pl.col('locale').is_in(train_target_locals))\n",
    "        .with_columns(pl.col('prev_items').apply(str2list))\n",
    "\n",
    ")\n",
    "test_data = (\n",
    "    pl.scan_parquet(os.path.join(base_dir,\n",
    "                                           raw_data_session_id_dir, \n",
    "                                           f'sessions_test_{task}.parquet'),\n",
    "                              n_rows=n_rows)\n",
    "    .with_columns(pl.col('prev_items').apply(str2list))\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3640939"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols_to_keep = ['prev_items']\n",
    "sentences = (\n",
    "    pl.concat([train_data.select(cols_to_keep),\n",
    "               eval_data.select(cols_to_keep),\n",
    "               train2_data.select(cols_to_keep),\n",
    "               test_data.select(cols_to_keep)], how='vertical')\n",
    ").collect().to_dict()['prev_items'].to_list()\n",
    "\n",
    "# sentences = map(sentences, list)\n",
    "\n",
    "\n",
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## W2V config "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0aJe9Y_uB8nF",
    "outputId": "e177ee22-154f-4195-f06a-6a7f7afba594"
   },
   "outputs": [],
   "source": [
    "vector_size = 32\n",
    "epochs = 30\n",
    "sg = 1 # 1 for skip-gram\n",
    "# pop_thresh = 0.82415\n",
    "window = 4\n",
    "min_count = 4\n",
    "\n",
    "# \n",
    "\n",
    "# len(sentences)sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "j2aLO74iLJzD"
   },
   "outputs": [],
   "source": [
    "from gensim.models.callbacks import CallbackAny2Vec\n",
    "\n",
    "class callback(CallbackAny2Vec):\n",
    "    '''Callback to print loss after each epoch.'''\n",
    "\n",
    "    def __init__(self):\n",
    "        self.epoch = 0\n",
    "        self.loss_to_be_subed = 0\n",
    "\n",
    "    def on_epoch_end(self, model):\n",
    "        loss = model.get_latest_training_loss()\n",
    "        loss_now = loss - self.loss_to_be_subed\n",
    "        self.loss_to_be_subed = loss\n",
    "        print('Loss after epoch {}: {}'.format(self.epoch, loss_now))\n",
    "        self.epoch += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZL3YXLg_Dzl3",
    "outputId": "558f7e2c-a427-41eb-a368-f12c08da34af"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 0: 12836455.0\n",
      "Loss after epoch 1: 5692851.0\n",
      "Loss after epoch 2: 3223340.0\n",
      "Loss after epoch 3: 2307628.0\n",
      "Loss after epoch 4: 1847326.0\n",
      "Loss after epoch 5: 1411062.0\n",
      "Loss after epoch 6: 1178932.0\n",
      "Loss after epoch 7: 927096.0\n",
      "Loss after epoch 8: 796098.0\n",
      "Loss after epoch 9: 670442.0\n",
      "Loss after epoch 10: 607928.0\n",
      "Loss after epoch 11: 554232.0\n",
      "Loss after epoch 12: 517214.0\n",
      "Loss after epoch 13: 462266.0\n",
      "Loss after epoch 14: 449234.0\n",
      "Loss after epoch 15: 244296.0\n",
      "Loss after epoch 16: 195824.0\n",
      "Loss after epoch 17: 197452.0\n",
      "Loss after epoch 18: 195060.0\n",
      "Loss after epoch 19: 181440.0\n",
      "Loss after epoch 20: 184524.0\n",
      "Loss after epoch 21: 176812.0\n",
      "Loss after epoch 22: 167788.0\n",
      "Loss after epoch 23: 167360.0\n",
      "Loss after epoch 24: 163064.0\n",
      "Loss after epoch 25: 165332.0\n",
      "Loss after epoch 26: 160448.0\n",
      "Loss after epoch 27: 161940.0\n",
      "Loss after epoch 28: 153096.0\n",
      "Loss after epoch 29: 151768.0\n"
     ]
    }
   ],
   "source": [
    "w2vec = Word2Vec(sentences=sentences, vector_size=vector_size, epochs = epochs, sg=sg,\n",
    "                 min_count=min_count, workers=14,\n",
    "                 window=window,\n",
    "                  compute_loss=True\n",
    "              , callbacks=[callback()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../model_training/w2v_task2_v2/True.model'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 100 trees are being used in this example\n",
    "annoy_index = AnnoyIndexer(w2vec, num_tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "id": "QKu7E6IcOHX1"
   },
   "outputs": [],
   "source": [
    "w2vec.save(w2v_model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../model_training/w2v_task2_v2/True.model'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "annoy_index.save(annoy_index_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../model_training/w2v_task2_v2/True.model'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../model_training/w2v_task2_v2/100_True.index'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annoy_index_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2vec = Word2Vec.load(w2v_model_file)\n",
    "annoy_index = AnnoyIndexer()\n",
    "annoy_index.load(annoy_index_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read new data  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DE', 'JP', 'UK']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_locals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train2_pl = pl.scan_parquet(os.path.join(base_dir, raw_data_session_id_dir, 'sessions_train2.parquet'), n_rows=n_rows).filter(pl.col('locale').is_in(target_locals)).with_columns(pl.col('prev_items').apply(str2list))\n",
    "\n",
    "eval_pl = pl.scan_parquet(os.path.join(base_dir, raw_data_session_id_dir, 'sessions_eval.parquet'), n_rows=n_rows).filter(pl.col('locale').is_in(target_locals)).with_columns(pl.col('prev_items').apply(str2list))\n",
    "\n",
    "test4task3_pl = pl.scan_parquet(os.path.join(base_dir, raw_data_session_id_dir, 'sessions_test_task3.parquet'), n_rows=n_rows).filter(pl.col('locale').is_in(target_locals)).with_columns(pl.col('prev_items').apply(str2list))\n",
    "test_pl = pl.scan_parquet(os.path.join(base_dir, raw_data_session_id_dir, f'sessions_test_{task}.parquet'), n_rows=n_rows).with_columns(pl.col('prev_items').apply(str2list))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DE', 'JP', 'UK']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_locals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_candidate_pl = w2v_rec(target_df=eval_pl,\n",
    "                   w2vec=w2vec, annoy_index=annoy_index, topn=topn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval_candidate_pl.head(1).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "eval_candidate_pl = eval_candidate_pl.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_candidate_pl.select('rec_num').describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_eval(target_df=eval_pl.join(eval_candidate_pl.lazy(), how='left', on='session_id'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# model_eval(target_df=target_df)\n",
    "eval_final = (\n",
    "        eval_pl.join(eval_candidate_pl.lazy(), how='left', on='session_id')\n",
    "        .lazy()\n",
    "        .with_columns(\n",
    "            pl.col('next_item_prediction').cast(pl.List(pl.Utf8))\n",
    "        )\n",
    "        .with_columns(\n",
    "            pl.concat_list([pl.col('next_item'), pl.col('next_item_prediction')]).alias('mrr')\n",
    "        )\n",
    "        .with_columns(\n",
    "            pl.col('mrr').arr.eval(\n",
    "                pl.arg_where(pl.element()==pl.element().first())\n",
    "            )\n",
    "        ).with_columns(\n",
    "            pl.col('mrr').arr.eval(\n",
    "                pl.when(pl.element()==0).then(0).otherwise(1/pl.element())\n",
    "            )\n",
    "        ).with_columns(\n",
    "            pl.col('mrr').arr.sum()\n",
    "            , pl.col('next_item_prediction').arr.head(20).arr.contains(pl.col('next_item')).mean().alias('recall@20')\n",
    "            , pl.col('next_item_prediction').arr.head(100).arr.contains(pl.col('next_item')).mean().alias('recall@100')\n",
    "            , pl.col('next_item_prediction').arr.head(topn).arr.contains(pl.col('next_item')).mean().alias('recall@all')\n",
    "\n",
    "\n",
    "        )\n",
    ")\n",
    "final_res = eval_final.select(\n",
    "        pl.count().alias('total_sessions')\n",
    "        , pl.col('mrr').mean()\n",
    "        , pl.col('recall@20').mean()\n",
    "        , pl.col('recall@100').mean()\n",
    "        , pl.col('recall@all').mean()\n",
    "\n",
    "    ).collect()\n",
    "final_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Candidate Saving "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train & eval  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_candidate_pl  = w2v_rec(target_df=train2_pl,\n",
    "                   w2vec=w2vec, annoy_index=annoy_index, topn=topn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 56s, sys: 1.3 s, total: 3min 58s\n",
      "Wall time: 3min 58s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_candidate_pl = train_candidate_pl.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150056, 3)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_candidate_pl.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_candidate_pl.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_candidate_pl.write_parquet(eval_cg_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_candidate_pl.write_parquet(train_cg_file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reprocess_data = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../data/candidates/task2_test_w2v_task2_v2_True_top100.parquet'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_cg_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task1_test_w2v_v3_True_top100_phase1.parquet\n",
      "task1_test_w2v_v3_True_top200.parquet\n",
      "task2_test_w2v_task2_True_top200.parquet\n",
      "task2_test_w2v_task2_v2_True_top100.parquet\n"
     ]
    }
   ],
   "source": [
    "! ls ../data/candidates | grep 'test_w2v'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 55 s, sys: 360 ms, total: 55.4 s\n",
      "Wall time: 55.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "test_candidate_pl  = w2v_rec(target_df=test_pl,\n",
    "                   w2vec=w2vec, annoy_index=annoy_index, topn=topn)\n",
    "test_candidate_pl = test_candidate_pl.collect()\n",
    "test_candidate_pl.write_parquet(test_cg_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test2task3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../data/candidates/task2_test4task3_w2v_task2_v2_True_top100.parquet'"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test4task3_cg_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 39.3 s, sys: 232 ms, total: 39.6 s\n",
      "Wall time: 39.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "test4task3_candidate_pl  = w2v_rec(target_df=test4task3_pl,\n",
    "                   w2vec=w2vec, annoy_index=annoy_index, topn=topn)\n",
    "test4task3_candidate_pl = test4task3_candidate_pl.collect()\n",
    "test4task3_candidate_pl.write_parquet(test4task3_cg_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "task2_eval_w2v_task2_v2_True_top100.parquet\n",
      "task2_test4task3_w2v_task2_v2_True_top100.parquet\n",
      "task2_test_w2v_task2_v2_True_top100.parquet\n",
      "task2_train_w2v_task2_v2_True_top100\n",
      "task2_train_w2v_task2_v2_True_top100.parquet\n"
     ]
    }
   ],
   "source": [
    "! ls ../data/candidates/ | grep w2v_task2_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def i2i(df):\n",
    "#     pl_df = pl.from_dataframe(df)\n",
    "#     pl_df = (\n",
    "#         pl_df\n",
    "#             .with_columns(pl.col('prev_items').apply(lambda row: get_rec(row, annoy_index=annoy_index, topn=100)).alias('next_item_prediction'))\n",
    "#     )\n",
    "#     return pl_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # %%time\n",
    "# def pd_get_rec(df, w2vec, annoy_index, topn):\n",
    "#     next_item_prediction_lst = []\n",
    "#     for a in tqdm(df.iterrows(), total=len(df)):\n",
    "#         prev_items = a[1]['prev_items']\n",
    "#         res = [ele.replace('[', '').replace(']', '').replace('\\n', '').replace(\"'\", '').replace(' ', '') for ele in prev_items.split(' ')]\n",
    "#         # print(type(res))\n",
    "#         similarity_dic = w2vec.wv.most_similar(positive=res, topn=topn, indexer=annoy_index)\n",
    "#         res = [item for item, simi in similarity_dic]\n",
    "#         next_item_prediction_lst.append(res)\n",
    "#     df['next_item_prediction'] = next_item_prediction_lst\n",
    "#     return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vNB90dXKlZkR",
    "tags": []
   },
   "source": [
    "# Validate predictions ✅ 😄"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = test_pl.join(test_candidate_pl.lazy(), how='left', on='session_id').collect()[['locale', 'next_item_prediction']].to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "FJA368Gzguk7"
   },
   "outputs": [],
   "source": [
    "check_predictions(predictions, test_sessions=test_pl.collect().to_pandas(), \n",
    "                  # check_products=True, product_df=products\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "1dTvU5VOgv0j"
   },
   "outputs": [],
   "source": [
    "# Its important that the parquet file you submit is saved with pyarrow backend\n",
    "predictions.to_parquet(submit_file, engine='pyarrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../data/sub_files/submit_task1_w2v_v3_True_top100.parquet'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submit_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dVrZ_TfnjL09",
    "tags": []
   },
   "source": [
    "## Submit to AIcrowd 🚀"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "rd9OYWEgixPZ"
   },
   "outputs": [],
   "source": [
    "# # You can submit with aicrowd-cli, or upload manually on the challenge page.\n",
    "# !aicrowd submission create -c task-1-next-product-recommendation -f {submit_file}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "als_model.pkl  itemid2item.json  user_item.npz\n"
     ]
    }
   ],
   "source": [
    "# ! ls {file_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval_pl.head().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "environment": {
   "kernel": "kdd_2023",
   "name": "common-cu110.m104",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cu110:m104"
  },
  "kernelspec": {
   "display_name": "py3.8(kdd_2023)",
   "language": "python",
   "name": "kdd_2023"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
