{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LmjiT10Qk5m8",
    "tags": []
   },
   "source": [
    "# Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "7DjmcQMAgPAJ"
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "import logging\n",
    "base_dir = '../'\n",
    "sys.path.append(base_dir)\n",
    "import os\n",
    "from gensim.similarities.annoy import AnnoyIndexer\n",
    "\n",
    "from utils import *\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gensim.models import Word2Vec\n",
    "from tqdm import tqdm\n",
    "import polars as pl\n",
    "from annoy import AnnoyIndex\n",
    "import polars as pl\n",
    "import polars as pl\n",
    "from utils import *\n",
    "from src.eval import model_eval\n",
    "from src.config import raw_data_session_id_dir, candidate_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JxcGbj4xAqqe"
   },
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "qYPjjtQ_AqRT"
   },
   "outputs": [],
   "source": [
    "debug = True\n",
    "\n",
    "if debug:\n",
    "    n_rows = 1000\n",
    "else:\n",
    "    n_rows = None\n",
    "\n",
    "train_data_dir = '.'\n",
    "test_data_dir = '.'\n",
    "task = 'task1'\n",
    "\n",
    "num_tree = 100\n",
    "\n",
    "model_dir = '../model_training/v2'\n",
    "\n",
    "# target locales: locales needed for task1\n",
    "\n",
    "target_locals = [\"DE\", 'JP', 'UK']\n",
    "\n",
    "w2v_model_file = os.path.join(model_dir, 'w2v.model')\n",
    "\n",
    "annoy_index_file = os.path.join(model_dir, f'annoy_index_{str(num_tree)}_trees.index')\n",
    "    \n",
    "# train_eval_result_file = os.path.join(eval_data_dir, 'train_result_w2v.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6oiTtQ56gYlY"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # %%time\n",
    "# def w2v_rec(df, w2vec, annoy_index, topn):\n",
    "#     next_item_prediction_lst = []\n",
    "#     for a in tqdm(df.iterrows(), total=len(df)):\n",
    "#         prev_items = a[1]['prev_items']\n",
    "#         res = list(prev_items)\n",
    "#         # print(res)\n",
    "#         # print(type(res))\n",
    "#         # print(res.shape)\n",
    "#         similarity_dic = w2vec.wv.most_similar(positive=res, topn=topn, indexer=annoy_index)\n",
    "#         res = [item for item, simi in similarity_dic]\n",
    "#         next_item_prediction_lst.append(res)\n",
    "#     df['next_item_prediction'] = next_item_prediction_lst\n",
    "#     return df\n",
    "\n",
    "def w2v_rec(target_df, w2vec, annoy_index, topn):\n",
    "    target_df = (\n",
    "        target_df.with_columns(\n",
    "            pl.col('prev_items').apply(lambda x: \n",
    "                                       # x.to_list()\n",
    "                                       list(map(list, zip(*w2vec.wv.most_similar(positive=x.to_list(),\n",
    "                                                                     topn=100,\n",
    "                                                                     indexer=annoy_index))))[0]\n",
    "                                      ).alias('next_item_prediction'))\n",
    "    )\n",
    "    return target_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WrMp8SO2tFtL",
    "tags": []
   },
   "source": [
    "# Data Description\n",
    "\n",
    "The Multilingual Shopping Session Dataset is a collection of **anonymized customer sessions** containing products from six different locales, namely English, German, Japanese, French, Italian, and Spanish. It consists of two main components: **user sessions** and **product attributes**. User sessions are a list of products that a user has engaged with in chronological order, while product attributes include various details like product title, price in local currency, brand, color, and description.\n",
    "\n",
    "---\n",
    "\n",
    "### Each product as its associated information:\n",
    "\n",
    "\n",
    "**locale**: the locale code of the product (e.g., DE)\n",
    "\n",
    "**id**: a unique for the product. Also known as Amazon Standard Item Number (ASIN) (e.g., B07WSY3MG8)\n",
    "\n",
    "**title**: title of the item (e.g., ‚ÄúJapanese Aesthetic Sakura Flowers Vaporwave Soft Grunge Gift T-Shirt‚Äù)\n",
    "\n",
    "**price**: price of the item in local currency (e.g., 24.99)\n",
    "\n",
    "**brand**: item brand name (e.g., ‚ÄúJapanese Aesthetic Flowers & Vaporwave Clothing‚Äù)\n",
    "\n",
    "**color**: color of the item (e.g., ‚ÄúBlack‚Äù)\n",
    "\n",
    "**size**: size of the item (e.g., ‚Äúxxl‚Äù)\n",
    "\n",
    "**model**: model of the item (e.g., ‚Äúiphone 13‚Äù)\n",
    "\n",
    "**material**: material of the item (e.g., ‚Äúcotton‚Äù)\n",
    "\n",
    "**author**: author of the item (e.g., ‚ÄúJ. K. Rowling‚Äù)\n",
    "\n",
    "**desc**: description about a item‚Äôs key features and benefits called out via bullet points (e.g., ‚ÄúSolid colors: 100% Cotton; Heather Grey: 90% Cotton, 10% Polyester; All Other Heathers ‚Ä¶‚Äù)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9ZEph_ZjlOj0"
   },
   "source": [
    "## EDA üíΩ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "f2L9ImDqge3_"
   },
   "outputs": [],
   "source": [
    "def read_locale_data(locale, task):\n",
    "    products = read_product_data().query(f'locale == \"{locale}\"')\n",
    "    sess_train = read_train_data().query(f'locale == \"{locale}\"')\n",
    "    sess_test = read_test_data(task).query(f'locale == \"{locale}\"')\n",
    "    return products, sess_train, sess_test\n",
    "\n",
    "def show_locale_info(locale, task):\n",
    "    products, sess_train, sess_test = read_locale_data(locale, task)\n",
    "\n",
    "    train_l = sess_train['prev_items'].apply(lambda sess: len(sess))\n",
    "    test_l = sess_test['prev_items'].apply(lambda sess: len(sess))\n",
    "\n",
    "    print(f\"Locale: {locale} \\n\"\n",
    "          f\"Number of products: {products['id'].nunique()} \\n\"\n",
    "          f\"Number of train sessions: {len(sess_train)} \\n\"\n",
    "          f\"Train session lengths - \"\n",
    "          f\"Mean: {train_l.mean():.2f} | Median {train_l.median():.2f} | \"\n",
    "          f\"Min: {train_l.min():.2f} | Max {train_l.max():.2f} \\n\"\n",
    "          f\"Number of test sessions: {len(sess_test)}\"\n",
    "        )\n",
    "    if len(sess_test) > 0:\n",
    "        print(\n",
    "             f\"Test session lengths - \"\n",
    "            f\"Mean: {test_l.mean():.2f} | Median {test_l.median():.2f} | \"\n",
    "            f\"Min: {test_l.min():.2f} | Max {test_l.max():.2f} \\n\"\n",
    "        )\n",
    "    print(\"======================================================================== \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tWCiG4Odggmo",
    "outputId": "e29c572c-b658-44e1-bbd5-c9df22dfded1"
   },
   "outputs": [],
   "source": [
    "# products = read_product_data(train_data_dir=train_data_dir)\n",
    "# # locale_names = products['locale'].unique()\n",
    "# # for locale in locale_names:\n",
    "# #     show_locale_info(locale, task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1551057, 11)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "products.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "S1S19qsFgk43",
    "outputId": "28e7f39f-a471-4f02-b763-bd89f15740dc"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prev_items</th>\n",
       "      <th>next_item</th>\n",
       "      <th>locale</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2687268</th>\n",
       "      <td>['B096Z9652W' 'B09WH4J2K7' 'B08Y71Y8YT']</td>\n",
       "      <td>B0927VPDKR</td>\n",
       "      <td>UK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1838192</th>\n",
       "      <td>['B00I95NJNS' 'B00IEA4OSW' 'B0872MYW83' 'B00I9...</td>\n",
       "      <td>B091CH9KKX</td>\n",
       "      <td>JP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>464603</th>\n",
       "      <td>['B0079F3ICQ' 'B0079F3KR4' 'B08VMMH3G7' 'B08VL...</td>\n",
       "      <td>B0842N3QGB</td>\n",
       "      <td>DE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3233842</th>\n",
       "      <td>['B095KL6H7J' 'B095KKM33R' 'B095KKSF72' 'B095K...</td>\n",
       "      <td>B087GTFVJF</td>\n",
       "      <td>UK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>414310</th>\n",
       "      <td>['B08FWYZL9V' 'B08FWWV3WH']</td>\n",
       "      <td>B08BGF2T1H</td>\n",
       "      <td>DE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                prev_items   next_item locale\n",
       "2687268           ['B096Z9652W' 'B09WH4J2K7' 'B08Y71Y8YT']  B0927VPDKR     UK\n",
       "1838192  ['B00I95NJNS' 'B00IEA4OSW' 'B0872MYW83' 'B00I9...  B091CH9KKX     JP\n",
       "464603   ['B0079F3ICQ' 'B0079F3KR4' 'B08VMMH3G7' 'B08VL...  B0842N3QGB     DE\n",
       "3233842  ['B095KL6H7J' 'B095KKM33R' 'B095KKSF72' 'B095K...  B087GTFVJF     UK\n",
       "414310                         ['B08FWYZL9V' 'B08FWWV3WH']  B08BGF2T1H     DE"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sessions = read_train_data(train_data_dir=train_data_dir)\n",
    "train_sessions.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "tbllmbaEgmBg",
    "outputId": "2560f079-ff99-4583-b38d-715f5fd14167"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prev_items</th>\n",
       "      <th>locale</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>69546</th>\n",
       "      <td>['B0B4JWSCFT' 'B09TT4LXWS']</td>\n",
       "      <td>DE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137545</th>\n",
       "      <td>['B07P6W92FK' 'B07P8KPPDC']</td>\n",
       "      <td>JP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221672</th>\n",
       "      <td>['B0BHMHV9Z3' 'B09Q934288' 'B09Q956R8R' 'B0B5X...</td>\n",
       "      <td>UK</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113348</th>\n",
       "      <td>['B0B3F181L1' 'B0BBLJ3D2K']</td>\n",
       "      <td>JP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162411</th>\n",
       "      <td>['B07XS4XN9V' 'B01MXLEVR7' 'B01N40PO2M' 'B07XP...</td>\n",
       "      <td>JP</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               prev_items locale\n",
       "69546                         ['B0B4JWSCFT' 'B09TT4LXWS']     DE\n",
       "137545                        ['B07P6W92FK' 'B07P8KPPDC']     JP\n",
       "221672  ['B0BHMHV9Z3' 'B09Q934288' 'B09Q956R8R' 'B0B5X...     UK\n",
       "113348                        ['B0B3F181L1' 'B0BBLJ3D2K']     JP\n",
       "162411  ['B07XS4XN9V' 'B01MXLEVR7' 'B01N40PO2M' 'B07XP...     JP"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sessions = read_test_data(task, test_data_dir=test_data_dir)\n",
    "test_sessions.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sessions = train_sessions[train_sessions['locale'].isin(target_locals)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "75D8eRsBBGQI"
   },
   "outputs": [],
   "source": [
    "\n",
    "if debug:\n",
    "    train_sessions = train_sessions.sample(debug_session_num)\n",
    "    test_sessions = test_sessions.sample(debug_session_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EEJ7k_ufBOe7",
    "outputId": "f28aacc0-097e-4b14-d0a5-28219acfe674"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3272716, 3)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sessions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "LpgqFwzADXJW"
   },
   "outputs": [],
   "source": [
    "train_sessions['prev_items'] = train_sessions.apply(lambda row: process_item_lst(row), axis=1)\n",
    "# test_sessions['prev_items'] = test_sessions.apply(lambda row: process_item_lst(row), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "locale\n",
       "UK    115936\n",
       "DE    104568\n",
       "JP     96467\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sessions['locale'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DE', 'JP', 'UK']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_locals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3272716, 3)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_sessions.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f0T1vtOEB5v-",
    "tags": []
   },
   "source": [
    "# Word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "FXQ0pY06CGiC"
   },
   "outputs": [],
   "source": [
    "# train_sessions['prev_items'].to_list()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model & annnoy index "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0aJe9Y_uB8nF",
    "outputId": "e177ee22-154f-4195-f06a-6a7f7afba594"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3589687"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_size = 32\n",
    "epochs = 10\n",
    "sg = 1 # 1 for skip-gram\n",
    "pop_thresh = 0.82415\n",
    "window = 4\n",
    "\n",
    "sentences = train_sessions['prev_items'].to_list() + test_sessions['prev_items'].to_list()\n",
    "len(sentences)\n",
    "\n",
    "\n",
    "from gensim.models.callbacks import CallbackAny2Vec\n",
    "\n",
    "class callback(CallbackAny2Vec):\n",
    "    '''Callback to print loss after each epoch.'''\n",
    "\n",
    "    def __init__(self):\n",
    "        self.epoch = 0\n",
    "        self.loss_to_be_subed = 0\n",
    "\n",
    "    def on_epoch_end(self, model):\n",
    "        loss = model.get_latest_training_loss()\n",
    "        loss_now = loss - self.loss_to_be_subed\n",
    "        self.loss_to_be_subed = loss\n",
    "        print('Loss after epoch {}: {}'.format(self.epoch, loss_now))\n",
    "        self.epoch += 1\n",
    "w2vec = Word2Vec(sentences=sentences, vector_size=vector_size, epochs = epochs, sg=sg,\n",
    "                 min_count=1, workers=14,\n",
    "                 window=window,\n",
    "                  compute_loss=True\n",
    "              , callbacks=[callback()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "j2aLO74iLJzD"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZL3YXLg_Dzl3",
    "outputId": "558f7e2c-a427-41eb-a368-f12c08da34af"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 0: 11067542.0\n",
      "Loss after epoch 1: 6493280.0\n",
      "Loss after epoch 2: 2888668.0\n",
      "Loss after epoch 3: 2580968.0\n",
      "Loss after epoch 4: 2329516.0\n",
      "Loss after epoch 5: 2048868.0\n",
      "Loss after epoch 6: 1821566.0\n",
      "Loss after epoch 7: 1773816.0\n",
      "Loss after epoch 8: 1667380.0\n",
      "Loss after epoch 9: 1025936.0\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../model_training/v2/w2v.model'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 100 trees are being used in this example\n",
    "annoy_index = AnnoyIndexer(w2vec, num_tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "QKu7E6IcOHX1"
   },
   "outputs": [],
   "source": [
    "w2vec.save(w2v_model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "annoy_index.save(annoy_index_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../model_training/v2/annoy_index_100_trees.index'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annoy_index_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yJoalTbYgnnp"
   },
   "source": [
    "Generate Submission üèãÔ∏è‚Äç‚ôÄÔ∏è\n",
    "\n",
    "\n",
    "\n",
    "Submission format:\n",
    "1. The submission should be a **parquet** file with the sessions from all the locales. \n",
    "2. Predicted products ids per locale should only be a valid product id of that locale. \n",
    "3. Predictions should be added in new column named **\"next_item_prediction\"**.\n",
    "4. Predictions should be a list of string id values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "B3zsYp_Jgn_J"
   },
   "outputs": [],
   "source": [
    "# def random_predicitons(locale, sess_test_locale):\n",
    "#     random_state = np.random.RandomState(42)\n",
    "#     products = read_product_data().query(f'locale == \"{locale}\"')\n",
    "#     predictions = []\n",
    "#     for _ in range(len(sess_test_locale)):\n",
    "#         predictions.append(\n",
    "#             list(products['id'].sample(PREDS_PER_SESSION, replace=True, random_state=random_state))\n",
    "#         ) \n",
    "#     sess_test_locale['next_item_prediction'] = predictions\n",
    "#     sess_test_locale.drop('prev_items', inplace=True, axis=1)\n",
    "#     return sess_test_locale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "cLu2KKKvF-ZA",
    "outputId": "b2d3da23-28cf-43dc-fd8f-d9e5ec16072d"
   },
   "outputs": [],
   "source": [
    "# test_sessions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 468
    },
    "id": "zZj7SRjpNuu0",
    "outputId": "dd576d00-b16b-4acd-e001-e8906e97d8d5"
   },
   "outputs": [],
   "source": [
    "# test_sessions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../model_training/v2/w2v.model'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2vec = Word2Vec.load(w2v_model_file)\n",
    "annoy_index = AnnoyIndexer()\n",
    "annoy_index.load(annoy_index_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pl = pl.scan_parquet(os.path.join(base_dir, raw_data_session_id_dir, 'sessions_train.parquet'), n_rows=n_rows).filter(pl.col('locale').is_in(target_locals)).with_columns(pl.col('prev_items').apply(str2list))\n",
    "eval_pl = pl.scan_parquet(os.path.join(base_dir, raw_data_session_id_dir, 'sessions_eval.parquet'), n_rows=n_rows).filter(pl.col('locale').is_in(target_locals)).with_columns(pl.col('prev_items').apply(str2list))\n",
    "\n",
    "# df_sess.head(3).collect()\n",
    "test_pl = pl.scan_parquet(os.path.join(base_dir, raw_data_session_id_dir, 'sessions_test_task1.parquet'), n_rows=n_rows).with_columns(pl.col('prev_items').apply(str2list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# eval_df = w2v_rec(df=eval_pl.collect().to_pandas(),\n",
    "#                    w2vec=w2vec, annoy_index=annoy_index, topn=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval_pl.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 113 ¬µs, sys: 17 ¬µs, total: 130 ¬µs\n",
      "Wall time: 135 ¬µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "eval_pl = w2v_rec(target_df=eval_pl,\n",
    "                   w2vec=w2vec, annoy_index=annoy_index, topn=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval_pl.head().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target_df = pl.from_pandas(eval_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target_df.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prev_items': Unknown,\n",
       " 'next_item': Utf8,\n",
       " 'locale': Utf8,\n",
       " 'session_id': Int64,\n",
       " 'next_item_prediction': Unknown}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_pl.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pl.List()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval_pl.with_columns(\n",
    "#     pl.col('next_item_prediction').cast(pl.List(pl.Utf8))\n",
    "# ).schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval_pl.collect().schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><style>\n",
       ".dataframe > thead > tr > th,\n",
       ".dataframe > tbody > tr > td {\n",
       "  text-align: right;\n",
       "}\n",
       "</style>\n",
       "<small>shape: (1, 3)</small><table border=\"1\" class=\"dataframe\"><thead><tr><th>mrr</th><th>recall@20</th><th>recall@100</th></tr><tr><td>f64</td><td>f64</td><td>f64</td></tr></thead><tbody><tr><td>0.034678</td><td>0.159</td><td>0.269</td></tr></tbody></table></div>"
      ],
      "text/plain": [
       "shape: (1, 3)\n",
       "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
       "‚îÇ mrr      ‚îÜ recall@20 ‚îÜ recall@100 ‚îÇ\n",
       "‚îÇ ---      ‚îÜ ---       ‚îÜ ---        ‚îÇ\n",
       "‚îÇ f64      ‚îÜ f64       ‚îÜ f64        ‚îÇ\n",
       "‚ïû‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï°\n",
       "‚îÇ 0.034678 ‚îÜ 0.159     ‚îÜ 0.269      ‚îÇ\n",
       "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_eval(target_df=eval_pl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Candidate Saving "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_pl.head().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = [\"B09SMK3R8H\", \"B01N4ND0F9\"]\n",
    "# w2vec.wv.most_similar(positive=x, topn=topn, indexer=annoy_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_pl.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# target_df.collect().schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # %%time\n",
    "# def pd_get_rec(df, w2vec, annoy_index, topn):\n",
    "#     next_item_prediction_lst = []\n",
    "#     for a in tqdm(df.iterrows(), total=len(df)):\n",
    "#         prev_items = a[1]['prev_items']\n",
    "#         res = [ele.replace('[', '').replace(']', '').replace('\\n', '').replace(\"'\", '').replace(' ', '') for ele in prev_items.split(' ')]\n",
    "#         # print(type(res))\n",
    "#         similarity_dic = w2vec.wv.most_similar(positive=res, topn=topn, indexer=annoy_index)\n",
    "#         res = [item for item, simi in similarity_dic]\n",
    "#         next_item_prediction_lst.append(res)\n",
    "#     df['next_item_prediction'] = next_item_prediction_lst\n",
    "#     return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 316971/316971 [13:38<00:00, 387.39it/s]\n"
     ]
    }
   ],
   "source": [
    "test_df = w2v_rec(df=test_pl.collect().to_pandas(),\n",
    "                   w2vec=w2vec, annoy_index=annoy_index, topn=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2946273/2946273 [2:12:06<00:00, 371.71it/s]  \n"
     ]
    }
   ],
   "source": [
    "train_df = w2v_rec(df=train_pl.collect().to_pandas(),\n",
    "                   w2vec=w2vec, annoy_index=annoy_index, topn=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_keep = ['session_id', 'next_item_prediction']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m cols_to_keep \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msession_id\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnext_item_prediction\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m----> 2\u001b[0m pl\u001b[38;5;241m.\u001b[39mfrom_pandas(\u001b[43mtrain_df\u001b[49m)\u001b[38;5;241m.\u001b[39mselect(cols_to_keep)\u001b[38;5;241m.\u001b[39mwrite_parquet(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(base_dir, candidate_dir, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtask1_train_w2v_top100.parquet\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m      3\u001b[0m pl\u001b[38;5;241m.\u001b[39mfrom_pandas(test_df)\u001b[38;5;241m.\u001b[39mselect(cols_to_keep)\u001b[38;5;241m.\u001b[39mwrite_parquet(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(base_dir, candidate_dir, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtask1_test_w2v_top100.parquet\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_df' is not defined"
     ]
    }
   ],
   "source": [
    "pl.from_pandas(train_df).select(cols_to_keep).write_parquet(os.path.join(base_dir, candidate_dir, 'task1_train_w2v_top100.parquet'))\n",
    "pl.from_pandas(test_df).select(cols_to_keep).write_parquet(os.path.join(base_dir, candidate_dir, 'task1_test_w2v_top100.parquet'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.from_pandas(eval_df).select(cols_to_keep).write_parquet(os.path.join(base_dir, candidate_dir, 'task1_eval_w2v_top100.parquet'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "ComputeError",
     "evalue": "ValueError: cannot do arithmetic with series of dtype: Utf8 and argument of type: <class 'str'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mComputeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 24\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res\n\u001b[1;32m     21\u001b[0m topn\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m\n\u001b[1;32m     23\u001b[0m (\n\u001b[0;32m---> 24\u001b[0m     \u001b[43mtarget_df\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwith_columns\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mprev_items\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mget_w2v_rec\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m )\u001b[38;5;241m.\u001b[39mhead()\u001b[38;5;241m.\u001b[39mcollect()\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/kdd-2023-KklMGVX0-py3.8/lib/python3.8/site-packages/polars/dataframe/frame.py:6798\u001b[0m, in \u001b[0;36mDataFrame.with_columns\u001b[0;34m(self, exprs, *more_exprs, **named_exprs)\u001b[0m\n\u001b[1;32m   6647\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwith_columns\u001b[39m(\n\u001b[1;32m   6648\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   6649\u001b[0m     exprs: IntoExpr \u001b[38;5;241m|\u001b[39m Iterable[IntoExpr] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   6650\u001b[0m     \u001b[38;5;241m*\u001b[39mmore_exprs: IntoExpr,\n\u001b[1;32m   6651\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mnamed_exprs: IntoExpr,\n\u001b[1;32m   6652\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Self:\n\u001b[1;32m   6653\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   6654\u001b[0m \u001b[38;5;124;03m    Add columns to this DataFrame.\u001b[39;00m\n\u001b[1;32m   6655\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   6795\u001b[0m \n\u001b[1;32m   6796\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   6797\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_from_pydf(\n\u001b[0;32m-> 6798\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlazy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6799\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwith_columns\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexprs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmore_exprs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mnamed_exprs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6800\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mno_optimization\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   6801\u001b[0m         \u001b[38;5;241m.\u001b[39m_df\n\u001b[1;32m   6802\u001b[0m     )\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/kdd-2023-KklMGVX0-py3.8/lib/python3.8/site-packages/polars/lazyframe/frame.py:1475\u001b[0m, in \u001b[0;36mLazyFrame.collect\u001b[0;34m(self, type_coercion, predicate_pushdown, projection_pushdown, simplify_expression, no_optimization, slice_pushdown, common_subplan_elimination, streaming)\u001b[0m\n\u001b[1;32m   1464\u001b[0m     common_subplan_elimination \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m   1466\u001b[0m ldf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ldf\u001b[38;5;241m.\u001b[39moptimization_toggle(\n\u001b[1;32m   1467\u001b[0m     type_coercion,\n\u001b[1;32m   1468\u001b[0m     predicate_pushdown,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1473\u001b[0m     streaming,\n\u001b[1;32m   1474\u001b[0m )\n\u001b[0;32m-> 1475\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrap_df(\u001b[43mldf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mComputeError\u001b[0m: ValueError: cannot do arithmetic with series of dtype: Utf8 and argument of type: <class 'str'>"
     ]
    }
   ],
   "source": [
    "# def w2v_rec(target_df, w2vec, annoy_index, topn=100):\n",
    "    \n",
    "#     next_item_prediction_lst = []\n",
    "#     for a in tqdm(df.iterrows(), total=len(df)):\n",
    "#         prev_items = a[1]['prev_items']\n",
    "#         res = [ele.replace('[', '').replace(']', '').replace('\\n', '').replace(\"'\", '').replace(' ', '') for ele in prev_items.split(' ')]\n",
    "#         # print(type(res))\n",
    "#         similarity_dic = w2vec.wv.most_similar(positive=res, topn=topn, indexer=annoy_index)\n",
    "#         res = [item for item, simi in similarity_dic]\n",
    "#         next_item_prediction_lst.append(res)\n",
    "#     df['next_item_prediction'] = next_item_prediction_lst\n",
    "    \n",
    "#     return df\n",
    "\n",
    "target_df = train_pl\n",
    "\n",
    "# def get_w2v_rec(x):\n",
    "#     res = w2vec.wv.most_similar(positive=x, topn=topn, indexer=annoy_index)\n",
    "#     return res\n",
    "\n",
    "get_vector = w2v.wv[pl.element()]\n",
    "\n",
    "topn=100\n",
    "\n",
    "(\n",
    "    target_df\n",
    "        .collect()\n",
    "        .with_columns(\n",
    "            pl.col('prev_items').apply(lambda x: get_w2v_rec(x=x))\n",
    "            )\n",
    ").head().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "mv6TZEFqgrFu"
   },
   "outputs": [],
   "source": [
    "# predictions = []\n",
    "# test_locale_names = test_sessions['locale'].unique()\n",
    "# for locale in test_locale_names:\n",
    "#     sess_test_locale = test_sessions.query(f'locale == \"{locale}\"').copy()\n",
    "#     predictions.append(\n",
    "#         random_predicitons(locale, sess_test_locale)\n",
    "#     )\n",
    "# predictions = pd.concat(predictions).reset_index(drop=True)\n",
    "# predictions.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "QBU2Y6P_IR5x"
   },
   "outputs": [],
   "source": [
    "# def get_predictions(row):\n",
    "#     prev_items = row['prev_items']\n",
    "#     # try:\n",
    "#     similarity_dic = w2vec.wv.most_similar(positive=prev_items, topn=100)\n",
    "#     res = [item for item, simi in similarity_dic] \n",
    "#         # print(err)\n",
    "#     # except Exception as e:\n",
    "#         # print(e)\n",
    "#     # res = prev_items\n",
    "#     return res "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # def get_session_vector(df, w2vec):\n",
    "# #   aids = df.aid.unique()\n",
    "# #   for i, aid in enumerate(aids):\n",
    "# #     vec = w2vec.wv[aid] if i == 0 else vec + w2vec.wv[aid]\n",
    "# #   vec = vec / len(aids)\n",
    "# #   return vec\n",
    "\n",
    "# # def get_close_aids(df, w2vec, index, idx2aid, n=20):\n",
    "# #   session_vec = get_session_vector(df, w2vec)\n",
    "# #   close_aids = get_nearest_neighbours(session_vec, index, idx2aid, n)\n",
    "# #   return close_aids\n",
    "\n",
    "# # def get_nearest_neighbours(x, index, idx2aid, n=20):\n",
    "# #   indexes, distances = index.get_nns_by_vector(x, n, search_k=-1, include_distances=True)\n",
    "# #   aids = [idx2aid[i] for i in indexes]\n",
    "# #   df = pd.DataFrame(data={'aid' : aids, 'w2vec_dist' : distances})\n",
    "# #   return df\n",
    "\n",
    "# index = AnnoyIndex(vector_size, distance)\n",
    "# aid2idx = {}\n",
    "\n",
    "# popular_aids = test.groupby('aid', as_index=False).agg({'session' : 'count'})\n",
    "# popular_aids = popular_aids.loc[popular_aids['session'] > popular_aids['session'].quantile(pop_thresh)]\n",
    "# popular_aid_list = popular_aids.aid.unique()\n",
    "\n",
    "# for i, aid in enumerate(popular_aid_list):\n",
    "# aid = str(aid)\n",
    "# aid2idx[aid] = i\n",
    "# index.add_item(i, w2vec.wv[aid])\n",
    "# idx2aid = { v : k for k, v in aid2idx.items()}\n",
    "# index.build(40) # build 40 trees\n",
    "\n",
    "# reduced_test = test.copy()\n",
    "# reduced_test['aid'] = reduced_test['aid'].astype('str')\n",
    "# reduced_test['aid_vector'] = reduced_test['aid'].apply(lambda x: w2vec.wv[x])\n",
    "\n",
    "# reduced_test = reduced_test.groupby('session').apply(lambda x: get_close_aids(x, w2vec, index, idx2aid, n)).reset_index().drop(columns='level_1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = test_sessions.sample(100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# df['next_item_prediction'] = df.apply(lambda row: get_predictions(row), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_sessions.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lst = ['B002ZCXPDU', 'B083MNDJLD', 'B08GR61FN6']\n",
    "\n",
    "# approximate_neighbors = w2vec.wv.most_similar(positive=lst, topn=5, indexer=annoy_index)\n",
    "# exact_neighbors = w2vec.wv.most_similar(positive=lst, topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# annoy_index_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Approximate: \")\n",
    "# print(approximate_neighbors)\n",
    "# print()\n",
    "# print('Exact: ')\n",
    "# print(exact_neighbors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rec(prev_items, topn=10, annoy_index=None):\n",
    "    # print(prev_items)\n",
    "    res = [ele.replace('[', '').replace(']', '').replace('\\n', '').replace(\"'\", '').replace(' ', '') for ele in prev_items.split(' ')]\n",
    "    # print(type(res))\n",
    "    try:\n",
    "        if annoy_index is not None:\n",
    "            similarity_dic = w2vec.wv.most_similar(positive=res, topn=topn, indexer=annoy_index)\n",
    "        else:\n",
    "            similarity_dic = w2vec.wv.most_similar(positive=res, topn=topn)\n",
    "        res = [item for item, simi in similarity_dic] \n",
    "        # print(err)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = test_sessions.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def i2i(df):\n",
    "    pl_df = pl.from_dataframe(df)\n",
    "    pl_df = (\n",
    "        pl_df\n",
    "            .with_columns(pl.col('prev_items').apply(lambda row: get_rec(row, annoy_index=annoy_index, topn=100)).alias('next_item_prediction'))\n",
    "    )\n",
    "    return pl_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "def pd_get_rec(df, w2vec, annoy_index, topn):\n",
    "    next_item_prediction_lst = []\n",
    "    for a in tqdm(df.iterrows(), total=len(df)):\n",
    "        prev_items = a[1]['prev_items']\n",
    "        res = [ele.replace('[', '').replace(']', '').replace('\\n', '').replace(\"'\", '').replace(' ', '') for ele in prev_items.split(' ')]\n",
    "        # print(type(res))\n",
    "        similarity_dic = w2vec.wv.most_similar(positive=res, topn=topn, indexer=annoy_index)\n",
    "        res = [item for item, simi in similarity_dic]\n",
    "        next_item_prediction_lst.append(res)\n",
    "    df['next_item_prediction'] = next_item_prediction_lst\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(316971, 2)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sessions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 338 ms, sys: 237 ms, total: 575 ms\n",
      "Wall time: 51.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pl_df = i2i(test_sessions.sample(100))\n",
    "result_df = pl_df.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_sessions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.06 ms, sys: 0 ns, total: 1.06 ms\n",
      "Wall time: 900 ¬µs\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 316971/316971 [40:26<00:00, 130.61it/s] \n"
     ]
    }
   ],
   "source": [
    "test_sessions = pd_get_rec(test_sessions, w2vec, annoy_index, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# similarity_dic[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 885 entries, 2866972 to 1610085\n",
      "Data columns (total 3 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   prev_items  885 non-null    object\n",
      " 1   next_item   885 non-null    object\n",
      " 2   locale      885 non-null    object\n",
      "dtypes: object(3)\n",
      "memory usage: 27.7+ KB\n"
     ]
    }
   ],
   "source": [
    "train_sessions.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pl_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# pl_df = pl.from_dataframe(df)\n",
    "# pl_df = (\n",
    "#     pl_df\n",
    "#         .with_columns(pl.col('prev_items').apply(lambda row: get_rec(row)).alias('next_item_prediction'))\n",
    "# )\n",
    "# result_df = pl_df.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(316971, 3)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "x7vA4qfcJDaW"
   },
   "outputs": [],
   "source": [
    "# test_sessions['next_item_prediction'] = test_sessions.apply(lambda row: get_predictions(row), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "id": "UXy4O2nrG6NC"
   },
   "outputs": [],
   "source": [
    "predictions = result_df[['locale', 'next_item_prediction']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "id": "ieI1oY58ICil"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>locale</th>\n",
       "      <th>next_item_prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>UK</td>\n",
       "      <td>[B08M2PY6J5, B07GDRHBKQ, B07JFH3QVK, B07ZKWQQ8...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>UK</td>\n",
       "      <td>[B0B61RHQWQ, B08FF24CPS, B09XTQCHMW, B08HJDGQD...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>UK</td>\n",
       "      <td>[B08MFDT65P, B079R741XF, B08MFH1TTJ, B08LQBJ9C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>JP</td>\n",
       "      <td>[B07BYFCVJP, B09Y1WTR2Z, B004O4C0RY, B07FPGLLL...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>UK</td>\n",
       "      <td>[1789083451, 0008534993, B08L9YYZSZ, 178294413...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  locale                               next_item_prediction\n",
       "0     UK  [B08M2PY6J5, B07GDRHBKQ, B07JFH3QVK, B07ZKWQQ8...\n",
       "1     UK  [B0B61RHQWQ, B08FF24CPS, B09XTQCHMW, B08HJDGQD...\n",
       "2     UK  [B08MFDT65P, B079R741XF, B08MFH1TTJ, B08LQBJ9C...\n",
       "3     JP  [B07BYFCVJP, B09Y1WTR2Z, B004O4C0RY, B07FPGLLL...\n",
       "4     UK  [1789083451, 0008534993, B08L9YYZSZ, 178294413..."
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 100 entries, 0 to 99\n",
      "Data columns (total 2 columns):\n",
      " #   Column                Non-Null Count  Dtype \n",
      "---  ------                --------------  ----- \n",
      " 0   locale                100 non-null    object\n",
      " 1   next_item_prediction  100 non-null    object\n",
      "dtypes: object(2)\n",
      "memory usage: 1.7+ KB\n"
     ]
    }
   ],
   "source": [
    "predictions.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 3272716 entries, 0 to 3272715\n",
      "Data columns (total 3 columns):\n",
      " #   Column      Dtype \n",
      "---  ------      ----- \n",
      " 0   prev_items  object\n",
      " 1   next_item   object\n",
      " 2   locale      object\n",
      "dtypes: object(3)\n",
      "memory usage: 99.9+ MB\n"
     ]
    }
   ],
   "source": [
    "train_sessions.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 300000/300000 [15:21<00:00, 325.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13min 24s, sys: 7.04 s, total: 13min 31s\n",
      "Wall time: 15min 21s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if debug:\n",
    "    num = 30\n",
    "else:\n",
    "    num = 30000\n",
    "train_df = pd_get_rec(df=train_sessions.sample(num), w2vec=w2vec, topn=100, annoy_index=annoy_index)\n",
    "# train_eval_df = pl_df.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len           100.00000\n",
      "recall@20       0.17188\n",
      "recall@100      0.29100\n",
      "dtype: float64\n",
      "CPU times: user 14.2 s, sys: 176 ms, total: 14.3 s\n",
      "Wall time: 14.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "eval_cols = ['len', 'recall@20', 'recall@100']\n",
    "train_df[eval_cols] = train_df.apply(pd_get_recall_at_k, axis=1, result_type='expand')\n",
    "print(train_df[eval_cols].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300000, 7)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.to_parquet(f'../data/eval_data/w2v_train_eval_result_300k.parquet', engine='pyarrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.apply(pd_get_recall_at_k, axis=1, result_type='expand')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vNB90dXKlZkR",
    "tags": []
   },
   "source": [
    "# Validate predictions ‚úÖ üòÑ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 316971 entries, 0 to 316970\n",
      "Data columns (total 3 columns):\n",
      " #   Column                Non-Null Count   Dtype \n",
      "---  ------                --------------   ----- \n",
      " 0   prev_items            316971 non-null  object\n",
      " 1   locale                316971 non-null  object\n",
      " 2   next_item_prediction  316971 non-null  object\n",
      "dtypes: object(3)\n",
      "memory usage: 7.3+ MB\n"
     ]
    }
   ],
   "source": [
    "test_sessions.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "id": "FJA368Gzguk7"
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Session ids of DE doesn't match",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[103], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mcheck_predictions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredictions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_sessions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_sessions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m                  \u001b[49m\u001b[38;5;66;43;03m# check_products=True, product_df=products\u001b[39;49;00m\n\u001b[1;32m      3\u001b[0m \u001b[43m                 \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/kdd_challenge_2023/notebooks/../utils.py:36\u001b[0m, in \u001b[0;36mcheck_predictions\u001b[0;34m(predictions, test_sessions, check_products, product_df)\u001b[0m\n\u001b[1;32m     34\u001b[0m sess_test \u001b[38;5;241m=\u001b[39m test_sessions\u001b[38;5;241m.\u001b[39mquery(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlocale == \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlocale\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     35\u001b[0m preds_locale \u001b[38;5;241m=\u001b[39m  predictions[predictions[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlocale\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m sess_test[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlocale\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m0\u001b[39m]]\n\u001b[0;32m---> 36\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(preds_locale\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39mvalues) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28msorted\u001b[39m(sess_test\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39mvalues), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSession ids of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlocale\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt match\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_products:\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;66;03m# This check is not done on the evaluator\u001b[39;00m\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;66;03m# but you can run it to verify there is no mixing of products between locales\u001b[39;00m\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;66;03m# Since the ground truth next item will always belong to the same locale\u001b[39;00m\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;66;03m# Warning - This can be slow to run\u001b[39;00m\n\u001b[1;32m     43\u001b[0m     products \u001b[38;5;241m=\u001b[39m product_df\u001b[38;5;241m.\u001b[39mquery(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlocale == \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlocale\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mAssertionError\u001b[0m: Session ids of DE doesn't match"
     ]
    }
   ],
   "source": [
    "check_predictions(predictions, test_sessions=test_sessions, \n",
    "                  # check_products=True, product_df=products\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1dTvU5VOgv0j"
   },
   "outputs": [],
   "source": [
    "# Its important that the parquet file you submit is saved with pyarrow backend\n",
    "predictions.to_parquet(f'submission_{task}.parquet', engine='pyarrow')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dVrZ_TfnjL09"
   },
   "source": [
    "## Submit to AIcrowd üöÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rd9OYWEgixPZ"
   },
   "outputs": [],
   "source": [
    "# You can submit with aicrowd-cli, or upload manually on the challenge page.\n",
    "!aicrowd submission create -c task-1-next-product-recommendation -f \"submission_task1.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "environment": {
   "kernel": "kdd_2023",
   "name": "common-cu110.m104",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cu110:m104"
  },
  "kernelspec": {
   "display_name": "py3.8(kdd_2023)",
   "language": "python",
   "name": "kdd_2023"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
